% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009

\documentclass{acm_proc_article-sp}
%\usepackage[pdftex]{graphicx}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subfig}
\DeclareCaptionType{copyrightbox}
\renewcommand{\algorithmiccomment}[1]{//#1}

\long\def\commentp#1{{\bf **P: #1**}}
\long\def\commentm#1{{\bf **M: #1**}}

\begin{document}

\title{A HyperNEAT-based Atari General Game Player}
%\subtitle{[Extended Abstract]
%% \titlenote{A full version of this paper is available as
%% \textit{Author's Guide to Preparing ACM SIG Proceedings Using
%% \LaTeX$2_\epsilon$\ and BibTeX} at
%% \texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

%% \numberofauthors{8} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
%% \author{
%% % You can go ahead and credit any number of authors here,
%% % e.g. one 'row of three' or two rows (consisting of one row of three
%% % and a second row of one, two or three).
%% %
%% % The command \alignauthor (no curly braces needed) should
%% % precede each author name, affiliation/snail-mail address and
%% % e-mail address. Additionally, tag each line of
%% % affiliation/address with \affaddr, and tag the
%% % e-mail address with \email.
%% %
%% % 1st. author
%% \alignauthor
%% Ben Trovato\titlenote{Dr.~Trovato insisted his name be first.}\\
%%        \affaddr{Institute for Clarity in Documentation}\\
%%        \affaddr{1932 Wallamaloo Lane}\\
%%        \affaddr{Wallamaloo, New Zealand}\\
%%        \email{trovato@corporation.com}
%% % 2nd. author
%% \alignauthor
%% G.K.M. Tobin\titlenote{The secretary disavows
%% any knowledge of this author's actions.}\\
%%        \affaddr{Institute for Clarity in Documentation}\\
%%        \affaddr{P.O. Box 1212}\\
%%        \affaddr{Dublin, Ohio 43017-6221}\\
%%        \email{webmaster@marysville-ohio.com}
%% % 3rd. author
%% \alignauthor Lars Th{\o}rv{\"a}ld\titlenote{This author is the
%% one who did all the really hard work.}\\
%%        \affaddr{The Th{\o}rv{\"a}ld Group}\\
%%        \affaddr{1 Th{\o}rv{\"a}ld Circle}\\
%%        \affaddr{Hekla, Iceland}\\
%%        \email{larst@affiliation.org}
%% \and  % use '\and' if you need 'another row' of author names
%% % 4th. author
%% \alignauthor Lawrence P. Leipuner\\
%%        \affaddr{Brookhaven Laboratories}\\
%%        \affaddr{Brookhaven National Lab}\\
%%        \affaddr{P.O. Box 5000}\\
%%        \email{lleipuner@researchlabs.org}
%% % 5th. author
%% \alignauthor Sean Fogarty\\
%%        \affaddr{NASA Ames Research Center}\\
%%        \affaddr{Moffett Field}\\
%%        \affaddr{California 94035}\\
%%        \email{fogartys@amesres.org}
%% % 6th. author
%% \alignauthor Charles Palmer\\
%%        \affaddr{Palmer Research Laboratories}\\
%%        \affaddr{8600 Datapoint Drive}\\
%%        \affaddr{San Antonio, Texas 78229}\\
%%        \email{cpalmer@prl.com}
%% }
%% % There's nothing stopping you putting the seventh, eighth, etc.
%% % author on the opening page (as the 'third row') but we ask,
%% % for aesthetic reasons that you place these 'additional authors'
%% % in the \additional authors block, viz.
%% \additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
%% email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
%% (The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
%% \date{30 July 1999}
%% % Just remember to make sure that the TOTAL number of authors
%% % is the number that will appear on the first page PLUS the
%% % number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
We use games as a platform for developing general intelligence, and therefore study methods that learn general game playing. 

We use the Atari as a platform for developing a general game playing agent. This agent could be considered a general intelligence and hopefully yield insights into algorithms and learning methods which are applicable to many tasks. Having generally applicable AI algorithms will lower the barrier of entry required to apply learning techniques to new problems. This may go a long way towards making AI more commonplace and pervasive.

In this work, we apply HyperNeat to the problem of learning to play Atari games based on visual input. By leveraging the geometric regularities present in the Atari game screen, HyperNeat is able to effectively evolve policies for playing two different Atari games. Results show that Hyperneat outperforms state of the art techniques.
\end{abstract}

% A category with the (minimum) three required fields
%% \category{H.4}{Information Systems Applications}{Miscellaneous}
%% %A category including the fourth, optional field follows...
%% \category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

%% \terms{Theory}

%% \keywords{ACM proceedings, \LaTeX, text tagging} % NOT required for Proceedings

\section{Introduction}
Games have long been considered a fruitful domain for the study of AI. Seminal work on game playing includes Samuel's checkers playing program\cite{samuel_59} and Tesauro's TD-Gammon\cite{tesauro_94}. The allure of games lies in the fact that they represent problems challenging enough to interest humans yet abstract enough to be easily captured and modeled inside of computer programs. Many traditional games such as Chess, Checkers, and Backgammon already have AI agents capable of outperforming human experts. However, new games are continually being created, many of which now incorporate sophisticated graphics and realistic physics. We focus on Atari 2600 games, a middle ground between classic board games and newer, graphically intensive video games. Atari games, like traditional board games, provide opportunities for agents to benefit from a solid understanding of the game's dynamics and allow for careful planning. At the same time these games, like modern video games, incorporate reasonably complex visual representations that need to be processed and interpreted.

Famous game playing intelligences such as Deep Blue for chess, Watson for Jeopardy, and TD-Gammon for backgammon all demonstrate that with enough manpower and ingenuity it is possible to tackle AI challenges which may have seemed insurmountable. Unlike these game intelligences which were created and tuned specifically for a single task, we aim to develop a single learning agent general enough to tackle many different Atari 2600 games. In doing so, we are forced to produce algorithms and techniques broadly applicable to tasks whose goals and dynamics may resemble anything from Checkers to Space Invaders.

Despite the variability of game dynamics, all Atari games share a standard interface designed for humans to interact with and enjoy. Game state is conveyed to the player through a 2D game screen, and in response, the player controls game elements by manipulating a four-directional joystick and a single button. This standard interface, combined with the large number of available games, makes the Atari a desirable platform for AI researchers. 

\begin{figure}[t]
\begin{center}
\includegraphics[width=.5\columnwidth]{figures/asterix.png}
\end{center}
\caption{Asterix, one of the many games available for the Atari 2600.}
\label{fig:asterix}
\end{figure}

While a number of methods could apply to Atari games, we develop an agent based around an evolutionary algorithm called Hypercube-based NeuroEvolution of Augmenting Topologies (HyperNEAT)~\cite{gauci08}. Unlike most other approaches, HyperNEAT is capable of exploiting geometric regularities present in the 2D game screen in order to evolve highly effective game playing policies. 

In the next section we discuss background and related work. Section \ref{sec:hyperneat} covers the basics of HyperNEAT and how it is able to take advantage of game geometry. Next, in Section \ref{sec:approach}, we present our visual processing architecture and parallel computing framework. Results are presented in Section \ref{sec:results}, followed by future work and conclusions.

\section{Background and Related Work}
\label{sec:background}
More challenging than learning to play a single game is creating a learning agent capable of playing a large number of games. Organizers in the field of General Game Playing (GGP) hold annual competitions for general game playing agents~\cite{genesereth05}. These agents are given a declarative description of an arbitrary game, of which they have no apriori knowledge, and must formulate strategies to play this game. Unlike specialized game players, general game players cannot rely on algorithms designed in advance for specific games. Successful agents typically incorporate artificial intelligence technologies such as knowledge representation, reasoning, learning, and rational decsion making. 

This work differs from GGP in that the games we learn to play are not abstract. Rather than a declarative representation of the game dynamics, we are presented only with on-screen objects. Furthermore, the dynamics of the game must first be learned before strategies can be formulated. Lastly, we currently only consider single player games as opposed to GGP players who compete against each other.

In contrast to the abstract representations of GGP tasks, annual Ms. Pacman competitions utilize actual screen representations~\cite{pacmancompetition}. Agents must deal with non-determinism introduced by delays in processing the game screen along with false object detections. Successful entries in this competition have now far exceeded novice human players \cite{sigevolution2007}. Our approach is similar to the Ms. Pac-man agents because we also extract objects from the game screen. However, since our approach must handle multiple games, we cannot utilize specialized object detection and game playing machinery.

Perhaps the first work on Atari game playing was an R-Max learning agent which employed an Object-oriented MDP representation~\cite{duik08}. Objects were detected in the game screen of the popular Atari game, \emph{Pitfall}. Results showed that the agent was able to make it past the first screen. Subsequent work on learning in Atari games includes a masters thesis by Yavar Naddaf at the University of Alberta~\cite{naddaf10}. In his thesis, Naddaf modified the popular Atari 2600 emulator, Stella, in order to allow it to be easily controlled by computer programs such as learning agents. Naddaf quantified the performance of several reinforcement learning and search agents over 50 different Atari games. Reinforcement learning agents included a gradient descent Sarsa$(\lambda)$ with linear function approximation which could learn from feature vectors generated from either the game screen or the console RAM. Search tree agents include full tree search and UCT based agents. Due to his extensive experimentation, his work represents a solid benchmark against which we compare our results.

Learning to play games based on overhead representations has been previously attempted by Verbancsics and Stanley~\cite{verbancsics10} who focused on the RoboCup Keepaway Soccer domain~\cite{stone01}, a task in which some number of \textit{keeper} agents must maneuver and pass a soccer ball so that it is not captured by one of the \textit{taker} agents. Verbancsics encodes the state of the game using an overhead representation of the objects on the playing field -- namely the keepers, takers, and the ball. In order to exploit the geometric regularities present in this overhead representation of the field, Verbancsics employs HyperNeat to learn a policy for playing this game. Results show that the learned policy is competitive with top learning algorithms for this task. Additionally, it is demonstrated that the learned policy can be effectively transferred with no further learning to the same task at a higher resolution or a different number of players on the field. This is a result of the indirect encoding present in HyperNeat. HyperNEAT has also been successfully applied to other domains such as checkers~\cite{gauci08}, multi-agent predator prey~\cite{ambrosio08}, and quadruped locomotion~\cite{clune09}. 

We apply the learning algorithm in Verbancsics' work, HyperNeat, to the domain of Atari games. While our approach is quite similar to Verbancsics', in many ways Atari games represent a more challenging learning target than RoboCup Keepaway. In Keepaway there are a fixed number of object classes such as takers, keepers, and the ball. On the other hand, Atari games may contain an arbitrary number of objects classes which interact with each other in unexpected ways, requiring our framework to be more general. Additionally, the dynamics in any given Atari game are highly variable, ranging from simple games in which the agent must reach the goal while avoiding cars to highly complex games in which the agent must shoot fish while attempting to rescue 5 swimmers, all before the oxygen in the player's submarine is depleted.

We will now dive into the specifics of the Atari 2600 simulator and what makes it such an attractive platform for learning agents.

\section{Atari for Research}
\label{sec:atari}
The Atari 2600 video game console was released in October 1977. Notably, it was credited with creating game cartridges which decoupled game code from console hardware. Previous devices all contained dedicated hardware with games already built in. Selling over 30 million consoles~\cite{atarihist}, Atari was considered wildly successful as an entertainment device. Today, while it's no longer at the forefront of entertainment, the Atari 2600 has good research potential for the following reasons:

First and foremost the Atari console has a large collection of games. These games vary greatly from board games such as chess to action-exploration games like Pitfall to shooting games such as Asteroids and Space Invaders. Many games have support for a second player, opening the possibility of multi-agent learning. Having such a large number of games allows AI researchers to develop a single learning agent and then quickly and easily apply it to a large set of domains.

In order to play these games, a number of open source Atari emulators exist, including projects such as Atari Learning Environment (ALE)\footnote{http://yavar.naddaf.name/ale/} which are designed specifically to accommodate learning agents. Furthermore, since the Atari 2600 CPU ran at 1.19 megahertz, modern emulators can run at high speeds of up to 2000 frames per second, making evaluation of agents and algorithms much quicker.

Additionally, the Atari state and action interface is simple enough for learning agents, but complex enough to control many different games. The state of an Atari game can be described relatively simply by its 2D graphics (containing between 8 and 256 colors depending on the color mode and a native resolution of 160x210), elementary sound effects, and 128 bytes of console RAM. The discrete action space for Atari consists of 4 possible directions of movement for the joystick and one button. Combinations of these two controls yield a total of 18 possible actions.

Having chosen the Atari as a suitable research platform, we turn next to the challenge of developing a capable HyperNeat-based learning agent.

\section{HyperNeat}
\label{sec:hyperneat}
In this section we review the fundamentals of the HyperNeat learning algorithm. HyperNEAT is an extension of the Neuro Evolution of Augmenting Topologies (NEAT) algorithm~\cite{stanley02}. NEAT evolves the topology and weights of an Artificial Neural Network (ANN) which is applied directly to the problem of interest. In contrast, HyperNEAT, introduced by Gauci and Stanley~\cite{gauci08}, evolves an \emph{indirect encoding} called a Compositional Pattern Producing Network (CPPN). The CPPN is then used to define the weights of an ANN which produces a solution for the problem. Furthermore, because the CPPN is aware of domain geometry, the ANNs it encodes implicitly contain knowledge about geometric relationships present in a given domain. This allows HyperNEAT to take advantage of geometric regularities present in many board and 2D games.

Specifically, HyperNEAT works in three stages:

\begin{enumerate}
\item HyperNEAT evolves the weights and topology of the CPPN. Internally a CPPN consists of functions such as Gaussians and Sinusoids connected in a weighted topology (see Figure \ref{fig:cppn}).
\item The CPPN is presented with a fully connected ANN and for every pair of \emph{(input,output)} nodes, determines the weight for that connection.
\item With fully specified weights, the ANN is applied directly to the problem of interest. Activations of output nodes are computed in a standard feed-forward manner.
\end{enumerate}



\begin{figure}[htp]
\begin{center}
\includegraphics[width=\columnwidth]{figures/cppn}
\end{center}
\caption{The relationship between a CPPN and its corresponding ANN. Because connection weights are produced as a function of a pair of nodes in the substrate (ANN) whose geometric relationship is known, geometric knowledge may be encoded into the ANN.}
%HyperNEAT evolves the weights and topology of a CPPN (right). This CPPN is subsequently used to determine all of the weights between substrate nodes in a fully connected ANN (left). Finally, the ANN is used to compute the solution to the desired problem. This three stage process form the core of HyperNEAT. CPPNs are said to be geometrically aware because when they compute the weights of the associated ANN, they are given the geometric location of both the input and output node in the ANN.}
\label{fig:cppn}
\end{figure}

Having covered the basics of HyperNEAT, we now discuss the specifics of our approach including game screen processing and self identification.

\section{Approach}
\label{sec:approach}
In this section we describe the details of our approach -- the main points are the manner in which the raw Atari game screen is processed to form an overhead representation amenable to HyperNEAT and the parallel framework used to speedup evolution.

\subsection{Visual Processing}
For nearly any machine learning problem, the question of how to encode the state space is of great importance. Following Verbancsics' example, we use an overhead object representation of the current game screen. Since we are given only the raw pixels of the screen as input, we design a simple visual processing stack which identifies objects and game entities without a priori knowledge of a specific game. A graphical depiction of this stack is shown in Figure \ref{fig:visproc}. Our approach borrows ideas from the original work of \cite{naddaf10}. 

\begin{figure*}[htp]
\begin{center}
\includegraphics[width=\textwidth]{figures/AtariArch}
\end{center}
\caption{Visual Processing Architecture applied to the game Freeway. Raw pixels from the game screen are displayed on the left. Next, contiguous pixels of the same color are merged into blobs. Objects are then extracted by merging adjacent blobs which exhibit constant velocity over the last two frames. Next, objects are clustered into object classes based on a pixel similarity score. Two main object classes are found -- cars facing left and cars facing right. Finally, self detection successfully identifies the chicken blob as the agent and colors it gray (circled in a red dashed line in rightmost screen).}
\label{fig:visproc}
\end{figure*}

Visual processing begins at the raw pixels of the game screen. We perform image segmentation in which adjacent raw pixels with similar colors are combined to form blobs. Next, blob merging occurs which outputs a set of current objects on screen. The process of blob merging examines all of the recently discovered blobs and compares them with equivalent blobs in the last frame in order to compute a velocity for each blob. Blobs are matched between screens using pixel similarity. Velocity is computed by measuring the displacement of blob centroids. Once each blob is assigned a velocity, adjacent blobs with the same non-zero velocity are merged into objects. Objects which are too small or become stationary are thrown out. This check helps to reduce the number of false positives in the object detection process.

Finally, objects are clustered into object classes or prototypes. Specifically, the shape of each pair of objects is compared and if found to exceed a similarity threshold (97\% pixel match in these experiments), the objects are grouped in the same class. Pixel similarity between two objects is computed by comparing the presence of pixels relative to each object's bounding box. As Figure \ref{fig:visproc} indicates, different object classes are discovered for the cars at the bottom half of the screen, cars at the top half of the screen, and the chicken. 

To reduce the number of spurious prototypes, prototypes are passed to the next stage of the approach only when 25 instances of that prototype have been seen without a gap of more than 3 frames. In the example in Figure \ref{fig:visproc}, this check helps removes prototypes for objects created when the cars are at the edges of the screen. Prototypes failing this check are removed while those which pass are assigned a unique real number (see Section \ref{sec:interface} for more details).

It is assumed here that objects belong to the same class if their shape is relatively similar without taking color into consideration. This has a potential drawback in certain games if different objects have similar shapes but different colors. 

\subsection{Self Identification}
The self detection step is meant to identify the location of an on-screen entity which is being controlled by the agent. In the vast majority of Atari games, the player's actions affect the movement of some on-screen entity, here termed the \textit{self}. Knowledge of the location of the \textit{self} is crucial to selecting an action, as described in below. We use an approach based on information gain to identify a blob most likely to correspond to the \textit{self}. Pseudocode is given in Algorithm \ref{alg:idself}.

\begin{algorithm}
\caption{Identify Self}
\label{alg:idself}
\begin{algorithmic}[1]
  \STATE $actions \leftarrow $ set of actions applicable to this game
  \STATE $current\_blobs \leftarrow $ set of blobs in the current game frame
  \STATE $ActionHistory \leftarrow \{a_0...a_n\}$ %\COMMENT{Actions at time 0...n}
  \FOR{blob $b \in current\_blobs$}
  \STATE $vHist_b \leftarrow \{v_0...v_n\}$ %\COMMENT{Velocity of blob $b$ at each timestep in the history}
  %\STATE \COMMENT{Information entropy of $b$'s velocity history}
  \STATE $H_b \leftarrow H(vHist_b)$ 
  \FOR{action $a \in actions$}
  %\STATE \COMMENT{set of $b$'s velocities for timesteps in which action $a$ was taken}
  \STATE $vHist_{(b|a)} \leftarrow [vHist_b[t] ~\forall_t: ActionHist[t-1] == a]$ 
  \STATE $H_{(b|a)} \leftarrow H(vHist_{(b|a)})$ %\COMMENT{Information entropy of $b$'s velocity history given action $a$ was taken}
  \ENDFOR
  \STATE $InfoGain_b \leftarrow H_b - sum_{a \in actions}(p_a * H_{(b|a)})$ %\COMMENT{$p_a$ is probability of action $a$ based on observed frequency} 
  \ENDFOR
  \RETURN $arg\_max_{b \in current\_blobs}(InfoGain_b)$ %\COMMENT{Return blob with max information gain}
\end{algorithmic}
\end{algorithm}

At the high level, we make certain assumptions about the self entity. Specifically, we expect that the self blob will move similarly whenever the same action is performed. That is, whenever an action, say Joystick Up, is taken, we expect the resulting velocity of the self blob to have a similar value (e.g. blob.y\_velocity = -1). 

As input, in lines 1-3, the algorithm has access to the set of possible joystick and button actions applicable to the current game (this is typically a subset of the 18 possible actions present on the Atari console), a list of blobs detected in the current frame, and a history of the actions taken by the agent. Additionally, in line 5, we have accesss to the $(x,y)$ velocity history $vHist$ of every blob. Next, we compute the entropy of blob $b$'s velocity history. Entropy is calculated using the standard formulation: $H(X) = -\sum_{i=1}^n{p(x_i)*lg(p(x_i))}$. Taking entropy over a velocity history involves computing the distribution over blob $b$'s velocity values. This is done using the empirically observed frequencies of each observed $(x,y)$ velocity value in $b$'s velocity history. A blob with highly random movement will exhibit a high information entropy over its full velocity history while a blob with highly regular movement will yield low entropy.

Having computed the information entropy over $b$'s full velocity history, we next examine each action individually and, in line 8, create $b$'s selective velocity history $vHist_{(b|a)}$. The selective velocity history simply filters the full velocity history by including only velocities which were observed in frames after which action $a$ was taken. For example, if $a = $ joystick left, then $vHist_{(b|a)}$ would only contain resultant velocities for frames in which $a$ was the action selected. In line 9, entropy is computed over $b$'s selective velocity history. This selective entropy $H_{(b|a)}$ should be low if it is the case that a given action reliably causes the blob to move in a certain direction.

\begin{figure*}
  \centering
  \subfloat[Object Classes]{\label{fig:classes}\includegraphics[width=0.196\textwidth]{figures/objClasses.png}}\hspace{.1in}
  \subfloat[Grid Overlay]{\label{fig:grid}\includegraphics[width=0.2\textwidth]{figures/grid-overlay.png}}\hspace{.1in}
  \subfloat[ANN Input]{\label{fig:anninput}\includegraphics[width=0.2\textwidth]{figures/ann-input.png}}\hspace{.1in}
  \subfloat[ANN Output]{\label{fig:annoutput}\includegraphics[width=0.2\textwidth]{figures/action-selection.png}}
 \caption{Interface between visual processing framework and the HyperNEAT ANN. Classes of objects are discretized into a grid whose cells are fed to the input nodes of the ANN via a map from object class to real number. After running the ANN, activations of the output layer in cells adjacent to the detected self are used to select which action the agent should take.}
 \label{fig:interface}
\end{figure*}

Finally, information gain is computed in line 11 by subtracting a frequency weighted sum of a blob's selective velocity entropies from the blob's full velocity entropy. If each of the selective entropies is low, as we expect for the self blob, this results in a high information gain. In line 13, the algorithm concludes by returning the blob with maximum information gain.

While this algorithm is generally successful in identifying the self blob, sometimes game dynamics break the assumption that actions result in similar velocities. For example, in the Freeway game, after colliding with a car, control is taken from the player and the chicken inadvertently moves down for several frames regardless of which actions the agent is executing. This results in irregular selective velocity histories and temporarily poorer identification of the self. However, in some sense the algorithm is correct in losing confidence in an object over which it no longer has control.

\subsection{Atari-HyperNEAT Interface}
\label{sec:interface}
After extracting objects classes as well as the location of the self from the raw game screen, this information needs to be sent to HyperNEAT. As discussed in Section \ref{sec:hyperneat}, HyperNEAT evolves a CPPN which encodes an ANN. In this section we assume we have a fully connected 2-layer ANN whose weights have been specified by the CPPN. At a high level, information from the game screen needs to be translated to activations of nodes in the input layer of the ANN. Then, after the network has been run in the standard feed-forward fashion, the activation of nodes on the output layer must be interpreted in order to select an action.

Figure \ref{fig:interface} shows an example of how objects classes are given as input to the substrate layer of the ANN. Since the ANN input nodes can only take real valued activations, each class of objects must be mapped to a real number. Thus we maintain a mapping from object classes to real values. Upon discovery of new object classes, real values are incrementally added to our map in a non-decreasing fashion. Next, since the ANN contains a 16x21 grid of input nodes, the raw game screen must also be discretized. Figure \ref{fig:grid} shows an example discretization of the game screen. Following this, each cell which contains an object is fed as input to the ANN with a real value corresponding to that object class's real value. Cells devoid of objects are given input activations of zero. The ANN is run in a standard feed-forward manner which produces activations of the nodes in the 16x21 output layer (Figure \ref{fig:annoutput}). In order to choose which action to take, we first locate the cell corresponding to the location of the detected self object (colored blue in Figure \ref{fig:annoutput}). Having located this cell, we next examine its activation as well as the activations of the four adjacent cells (shown with red arrows) and select an action corresponding to the arrow in the highest of these five cells, or no-op action if the self square has the highest value. This allows us to select one of five possible actions. Future work involves extending this framework to support more complex actions such a button presses and combinations of joystick and button press actions.

Having described the interface between our visual processing framework and the HyperNEAT agent, we focus next on parallelizing the evaluation of individuals.

\subsection{Parallel Implementation}
\label{sec:condor}
The visual processing and self identification components of our code require some amount of processing time each frame. Depending on the game being played, evaluating a single individual can take anywhere between a few seconds to 5 minutes. Based on some initial experimental work, we estimated that we would need to run HyperNEAT for \textit{at least} 250 generations with 100 individuals per generation to obtain representative results. These parameters make it infeasible to run the entire computation on a single machine in a reasonable amount of time. To speed up evolution, we exploit the embarrassingly parallel nature of this approach by evaluating each individual on a separate machine. We make use of the Condor distributed processing cluster \cite{thain2005distributed} to run a massive number of Atari simulations in parallel. A similar framework has been used in the past to speed up the optimization of robot soccer agents in simulation using CMA-ES\cite{Urieli+MKBS:2010}. We have modified the original HyperNEAT implementation of \cite{verbancsics10} to run on the Condor cluster.

% To this end, we exploit the embarassingly parallel nature of the evaluating a number of individuals in a single generation. We use the Depending on the game being played in the Atari simulator, a single evaluation of an individual can last from anything between a few seconds to 2 or 3 minutes. While keeping the evolution parameters to as low as 100 individuals and 300 generations, it is still not possible to obtain results in sufficient time on a single machine. To mitigate this problem, we exploit the embarassingly parallel nature of evaluating individuals. We have modifed the HyperNEAT codebase to run on the Condor cluster, by adapting the framework developed by Yinon Bentor and Patrick MacAlpine.\footnote{This framework was initially developed in a previous iteration of the CS394N class. Since then it has been used by the AustinVilla Robot Soccer team for optimizing policy parameters for various soccer related tasks using evolutionary strategies such as CMA-ES.}

% \begin{figure*}[ht]
% \begin{center}
% \includegraphics[width=\textwidth]{figures/CondorHyperNEAT}
% \end{center}
% \caption{Procedure for using HyperNEAT on the condor framework. An initial Master process is responsible for running NEAT and prodcuding successive generations. Condor jobs are initialized with each job being provided the population file and the id of th individual it is responsible for evaluating. Each job starts an instance of the Atari emulator and proceeds to evaluate the individual by playing the game using the policy defined by that individual. A fitness value is calculated, corresponding to the score obtained in the game and written out to file. This is read back by the Master process which then proceeds to use the fitness values to produce the next generation.}
% \label{fig:condor}
% \end{figure*}

\begin{figure}[ht]
\begin{center}
\includegraphics[width=\columnwidth]{figures/condor-hyperneat-small.png}
\end{center}
\caption{Parallel evaluations on the Condor framework.}
\label{fig:condor}
\end{figure}

Figure \ref{fig:condor} details how parallel evaluations are performed using the Condor cluster. Once a generation is produced, all the individuals of that generation are evaluated on separate machines in parallel. After all evaluations have been completed, the fitness values are gathered and a new generation is produced. This process is then repeated for each successive generation. 

We detected a practical problem while using the framework; certain fitness evaluations do not complete and are assigned the minimum score. As a result, some individuals may be lost before maturing enough to improve other members of the population. Although such a situation is not ideal, we believe the evolutionary process is robust enough to compensate. This hypothesis is corroborated by the steady increase of the average champion fitness in our experiments (see Fig. \ref{fig:asterix-curve}). 

\section{Results}
\label{sec:results}
In this section, we provide results obtained from applying HyperNEAT on two Atari games, \textit{Freeway} and \textit{Asterix}. The experimental setup was the same for both Freeway and Asterix: HyperNEAT was run for 250 generations, with 100 individuals in each generation and a substrate resolution of 16x21. All individuals were evaluated on the Atari simulator using the same random seed. We averaged results across multiple runs of the HyperNEAT evolution and compare them with previous results obtained using reinforcement learning \cite{naddaf10}. In Table 1, we present the fitness values of the best individual averaged across all the runs, as well as the overall best individual found. BASS, DISCO, and RAM are gradient descent Sarsa$(\lambda)$ agents with linear function approximation. These methods use state representations corresponding to the sub-sampled screen image, objects on screen, and console ram respectively. We also compare against results obtained from an agent taking random actions each step \cite{naddaf10}. 

\subsection {Freeway}

Freeway is an Atari game similar to the arcade game \textit{Frogger}. The player controls a chicken with the objective of reaching the top of the highway while avoiding cars (see Fig. \ref{fig:visproc}). The player receives a score of 1 if the chicken reaches the top of the highway, at which point the chicken moves back to the bottom of the screen. If the chicken gets hit by a car, it gets pushed down by a fixed small distance. The game ends after two minutes of game-play. There are three possible actions that the player may use in Freeway: \textit{Up}, \textit{Down}, and \textit{No-Action}.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=\columnwidth]{figures/freeway-results.png}
\end{center}
\caption{HyperNEAT learning performance on the Freeway game. Average fitness of the population along with the champion fitness for each generation are displayed. Error bars represent standard deviation. The fitness of an individual corresponds exactly to their game score.}
\label{fig:freeway-curve}
\end{figure}

\commentm{Change graph style so that we can differentiate between the two lines in b/w.}

% \ref{tab:results-table} points incorrectly to 6?
Average fitness of the population and the champion fitness throughout the learning process can be seen in Fig. \ref{fig:freeway-curve}. Results were averaged across 5 runs of HyperNEAT evolution. Table 1 contrasts our approach with previous results. The most striking observation from these results is that reinforcement learning is unable to find a solution to the problem, whereas even the first generation in HyperNEAT produces a champion with a very high fitness (starting average champion fitness = 26.4). Reinforcement learning has to perform a large number of exploratory actions before it can reach the goal state at the top, and succumbs to the highly delayed nature of the reward in this game. On the other hand, all individuals in HyperNEAT are produced by randomly initialized CPPNs. Due to the nature of CPPNs, at least a few individuals have the propensity to always take the \textit{Up} action and obtain a large fitness at the outset. Evolution helps individuals to learn fairly quickly to avoid cars over the next 50 generations, which can be seen by the slight increase in champion and average fitness.

\subsection {Asterix}
In Asterix, the player controls a unit called Asterix with the objective of collecting magic potions and avoiding \textit{lyres}. Both the magic potions and the lyres move across the screen around Asterix. Each time Asterix collects a magic potion, the player receives a score of 50. The game is over when the Asterix touches a lyre. Asterix can take five possible actions: \textit{Up}, \textit{Down}, \textit{Left}, \textit{Right}, and \textit{No-Action}.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=\columnwidth]{figures/asterix-results.png}
\end{center}
\caption{HyperNEAT learning performance on the Asterix game. The average fitness of the population along with the champion at each generation in the Asterix game. Error bars represent standard deviation. Fitness of an individual corresponds exactly to their game score.}
\label{fig:asterix-curve}
\end{figure}

Average fitness of the population and the champion fitness throughout the learning process can be seen in Fig. \ref{fig:asterix-curve}. Results for Asterix were averaged across 10 runs of HyperNEAT evolution. Table 1 contrasts our approach with previous results. The results for Asterix are qualitatively different from Freeway in a number of ways:
\begin{itemize}
\item
Random exploration obtains non-zero reward in Asterix as Asterix inadvertently collects magic potions during random motion. Reinforcement Learning approaches can bootstrap from this information and learn to become statistically better than random.
\item
Champions from HyperNEAT evolution start at close to random performance (starting average champion fitness = 190) and improve their performance to the same level as that of reinforcement learning approaches within 50 generations. 
\item
The learning process steadily improves the fitness through the entirety of 250 generations. This demonstrates the power of using CPPNs at representing good policies for this game. Secondly, it also shows the ability of HyperNEAT to evolve better CPPNs.
\end{itemize}

%Results from the Asterix game are summarized in Figure \ref{fig:asterix-curve} and Table 1. Unlike the Freeway game, the champion fitness curve shows significant improvement accross successive generations. Secondly, the champion fitness reaches a high of 750 before dropping down to around the 550 mark. We suspect that our Condor implementaion is responsible, and that the champion was lost to an incomplete Condor job (see Section \ref{sec:condor}). We hope to rectify this issue in the future by making our framework more robust. In contrast to the Freeway game, RL methods perform decently well in this domain. Initial exploration probably causes the agent to learn to stay away from the sprites much faster in this case. However HyperNEAT still outperforms RL based methods. On the other hand, HyperNEAT does not perform as well as a human player. We attribute this to the higher complexity of the game, as well as multiple object classes (which is something we are still working on). 

\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
~ & \textbf{Freeway} & \textbf{Asterix} \\ \hline
\textbf{BASS} & 0 & 402 \\ \hline
\textbf{DISCO} & 0 & 301 \\ \hline
\textbf{RAM} & 0 & 545 \\ \hline
\textbf{Random} & 0 & 156 \\ \hline
\textbf{HyperNEAT(Average)} & 28.4 & 725 \\ \hline
\textbf{HyperNEAT(Best)} & 29 & 1000 \\ 
\hline
\end{tabular}
\end{center}
\label{tab:results-table}
\caption{Game scores obtained in the Freeway and Asterix games.}
\end{table}

Our results show excellent performance by HyperNEAT on the Asterix and Freeway games. However, to extend HyperNEAT to play arbitrary games in the Atari simulator, some future work is required.

\section{Future Work}
\label{sec:futurework}
First and foremost, we hope to make the framework applicable to a larger set of games. There are two main challenges in this regard -- first handling games with large numbers of possible actions and second handling games with many different object classes.

In Freeway and Asterix, like in Robocup Keepaway, there are relatively few classes of objects which matter: cars and the chicken for Freeway, potions and lyres for Asterix, and takers and keepers for keepaway. Having a limited number of object classes permits an easy mapping from objects to substrate values. For example, in Keepaway, keepers can be assigned values of 1 and takers values of -1. Having such few values allows HyperNEAT to easily differentiate different classes of objects and exhibit appropriate behaviors with respect to each object class, behaviors such as avoiding lyres and collecting potions.

As the nubmer of object classes increases, the map of object class to substrate values grows more crowded. In the worst case, if two opposite classes of objects (e.g. keepers and takers) get mapped to the same real value, HyperNEAT will be unable to distinguish between them and formulate appropriate strategies for dealing with each class.

One possible solution to this problem is to create input substrates, with one substrate for each class of objects, as seen in Figure \ref{fig:possiblearch}. Each new object class is given its own substrate layer which it exclusively populates. At runtime, each of the substrate layers is connected to the processing layer and run in a feed-forward manner. To accommodate multiple substrates, an additional input needs to be added to the CPPN to specify which substrate is currently active. In this way, HyperNEAT would, with some additional computation cost, be able to encode an entirely different policy for an arbitrary number of object classes. 

The second area of future work involves developing a better way to handle a large number of actions. In games like Freeway in which there are only a few actions (up, down, no-op), we can choose which action to take by examining values of the output nodes adjacent to the self node (as described in Section \ref{sec:interface}). This becomes more complicated when other actions such as button presses are involved. For example, which node's value should be examined to decide if the button press action should be taken? To address this issue, we hope to add another output layer above the processing layer. This additional output layer would have a single node for each of the possible actions in the game. Action selection would simply reduce to finding the action node with the highest value. If additional actions need to be introduced, more action nodes can be created in the output layer. The top portion of Figure \ref{fig:possiblearch} depicts this alternative architecture.

\begin{figure}[htp]
\begin{center}
\includegraphics[width=\columnwidth]{figures/multiple-substrate.png}
\end{center}
\caption{Alternative architecture for handling variable numbers of object classes and actions. New object classes are assigned individual substrate layers. Additionally, above the processing level exists a node for each possible action. At runtime activations are propagated up from all $N$ substrate layers to the processing and action layers. Action selection involves picking which action node has the highest activation.}
\label{fig:possiblearch}
\end{figure}

\section{Conclusions}
\label{sec:conclusion}
In this work we have applied HyperNEAT to the domain of Atari games. Many Atari games contain geometric regularities in the two-dimensional space of the game screen. This allows HyperNEAT to quickly learn effective policies. To reduce the complexity of learning from the raw game screen, we introduce a game-independent visual processing hierarchy designed to identify classes of objects as well as the entity that the player controls on the game screen. Identified objects are provided as input to HyperNEAT. Due to the computational overhead of visual processing applied to each game screen, we utilize a parallel architecture to simultaneously evaluate multiple individuals. Results are presented for two Atari games, \textit{Freeway} and \textit{Asterix}. In both cases, HyperNEAT is shown to outperform previous benchmarks~\cite{naddaf10}. While no single Atari game, if studied in isolation and given extensive feature engineering, likely poses too great a challenge for modern AI techniques, the full collection of over 900 Atari games presents a daunting task for a single learning agent. It is this goal of creating an agent capable of learning and seamlessly transitioning between many different tasks to which we strive.

%\end{document}  % This is where a 'short' article might terminate

%ACKNOWLEDGMENTS are optional
%% \section{Acknowledgments}
%% This section is optional; it is a location for you
%% to acknowledge grants, funding, editing assistance and
%% what have you.  In the present case, for example, the
%% authors would like to thank Gerald Murray of ACM for
%% his help in codifying this \textit{Author's Guide}
%% and the \textbf{.cls} and \textbf{.tex} files that it describes.

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{hyperneat}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references

\end{document}
