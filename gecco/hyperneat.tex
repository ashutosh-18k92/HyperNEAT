% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009

\documentclass{acm_proc_article-sp}
%\usepackage[pdftex]{graphicx}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subfig}
\DeclareCaptionType{copyrightbox}
\renewcommand{\algorithmiccomment}[1]{//#1}

\long\def\commentp#1{{\bf **P: #1**}}
\long\def\commentm#1{{\bf **M: #1**}}

\begin{document}

\title{A HyperNEAT-based Atari General Game Player}
%\subtitle{[Extended Abstract]
%% \titlenote{A full version of this paper is available as
%% \textit{Author's Guide to Preparing ACM SIG Proceedings Using
%% \LaTeX$2_\epsilon$\ and BibTeX} at
%% \texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

%% \numberofauthors{8} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
%% \author{
%% % You can go ahead and credit any number of authors here,
%% % e.g. one 'row of three' or two rows (consisting of one row of three
%% % and a second row of one, two or three).
%% %
%% % The command \alignauthor (no curly braces needed) should
%% % precede each author name, affiliation/snail-mail address and
%% % e-mail address. Additionally, tag each line of
%% % affiliation/address with \affaddr, and tag the
%% % e-mail address with \email.
%% %
%% % 1st. author
%% \alignauthor
%% Ben Trovato\titlenote{Dr.~Trovato insisted his name be first.}\\
%%        \affaddr{Institute for Clarity in Documentation}\\
%%        \affaddr{1932 Wallamaloo Lane}\\
%%        \affaddr{Wallamaloo, New Zealand}\\
%%        \email{trovato@corporation.com}
%% % 2nd. author
%% \alignauthor
%% G.K.M. Tobin\titlenote{The secretary disavows
%% any knowledge of this author's actions.}\\
%%        \affaddr{Institute for Clarity in Documentation}\\
%%        \affaddr{P.O. Box 1212}\\
%%        \affaddr{Dublin, Ohio 43017-6221}\\
%%        \email{webmaster@marysville-ohio.com}
%% % 3rd. author
%% \alignauthor Lars Th{\o}rv{\"a}ld\titlenote{This author is the
%% one who did all the really hard work.}\\
%%        \affaddr{The Th{\o}rv{\"a}ld Group}\\
%%        \affaddr{1 Th{\o}rv{\"a}ld Circle}\\
%%        \affaddr{Hekla, Iceland}\\
%%        \email{larst@affiliation.org}
%% \and  % use '\and' if you need 'another row' of author names
%% % 4th. author
%% \alignauthor Lawrence P. Leipuner\\
%%        \affaddr{Brookhaven Laboratories}\\
%%        \affaddr{Brookhaven National Lab}\\
%%        \affaddr{P.O. Box 5000}\\
%%        \email{lleipuner@researchlabs.org}
%% % 5th. author
%% \alignauthor Sean Fogarty\\
%%        \affaddr{NASA Ames Research Center}\\
%%        \affaddr{Moffett Field}\\
%%        \affaddr{California 94035}\\
%%        \email{fogartys@amesres.org}
%% % 6th. author
%% \alignauthor Charles Palmer\\
%%        \affaddr{Palmer Research Laboratories}\\
%%        \affaddr{8600 Datapoint Drive}\\
%%        \affaddr{San Antonio, Texas 78229}\\
%%        \email{cpalmer@prl.com}
%% }
%% % There's nothing stopping you putting the seventh, eighth, etc.
%% % author on the opening page (as the 'third row') but we ask,
%% % for aesthetic reasons that you place these 'additional authors'
%% % in the \additional authors block, viz.
%% \additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
%% email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
%% (The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
%% \date{30 July 1999}
%% % Just remember to make sure that the TOTAL number of authors
%% % is the number that will appear on the first page PLUS the
%% % number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
This paper focuses on developing general intelligence through the study of game playing agents. The main contribution is HyperNEAT-GGP, a HyperNEAT-based approach to playing general Atari games using a visual representation of the game screen. By leveraging the geometric regularities present in the Atari game screen, the HyperNeat effectively evolves policies for playing two different Atari games, Asterix and Freeway. Results show that the HyperNEAT-GGP outperforms state of the art techniques on these games. In creating a general game player, this work exposes some of the challenges inherent to the study of general intelligence. If such challenges can be overcome, generally intelligent agents have potential to lower the threshold of knowledge and expertise required to apply AI to new problems.
\end{abstract}

% A category with the (minimum) three required fields
%% \category{H.4}{Information Systems Applications}{Miscellaneous}
%% %A category including the fourth, optional field follows...
%% \category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

%% \terms{Theory}

%% \keywords{ACM proceedings, \LaTeX, text tagging} % NOT required for Proceedings

\section{Introduction}
Games have long been considered a fruitful domain for the study of AI. Seminal work on game playing includes Samuel's checkers playing program\cite{samuel_59} and Tesauro's TD-Gammon\cite{tesauro_94}. The allure of games lies in the fact that they represent problems challenging enough to interest humans yet abstract enough to be easily captured and modeled inside of computer programs. Many traditional games such as Chess, Checkers, and Backgammon already have AI agents capable of outperforming human experts. However, new games are continually being created, many of which now incorporate sophisticated graphics and realistic physics. This work focuses on Atari 2600 games, a middle ground between classic board games and newer, graphically intensive video games. Atari games, like traditional board games, provide opportunities for agents to benefit from a solid understanding of the game's dynamics and allow for careful planning. At the same time these games, like modern video games, incorporate reasonably complex visual representations that need to be processed and interpreted.

Famous game playing AI systems such as Deep Blue for chess, Watson for Jeopardy, and TD-Gammon for backgammon all demonstrate that with enough manpower and ingenuity it is possible to tackle AI challenges that may have previously seemed insurmountable. Unlike these game intelligences which were created and tuned specifically for a single task, the game playing agent described herein must be general enough to tackle many different Atari 2600 games, necessitating the use of algorithms and techniques broadly applicable to games whose goals and dynamics may resemble anything from Checkers to Space Invaders. 

Scientifically the aim of this work is to develop a learning agent capable of playing a large number of Atari games from a common visual input representation. The motivation behind this goal is the study general intelligence through the domain of general game players. 

Despite the variability of game dynamics, all Atari games share a standard interface designed for humans to interact with and enjoy. Game state is conveyed to the player through a 2D game screen, and in response, the player controls game elements by manipulating a four-directional joystick and a single button. This standard interface, combined with the large number of available games, makes Atari a convenient platform for AI researchers. 

\begin{figure}[t]
\begin{center}
\includegraphics[width=.5\columnwidth]{figures/asterix.png}
\end{center}
\caption{Asterix, one of the many games available for the Atari 2600.}
\label{fig:asterix}
\end{figure}

This paper presents an agent based around an evolutionary algorithm called Hypercube-based NeuroEvolution of Augmenting Topologies (HyperNEAT)~\cite{gauci08}. Unlike most other approaches, HyperNEAT is capable of exploiting geometric regularities present in the 2D game screen in order to evolve highly effective game playing policies. 

In the next section contains background and related work. Section \ref{sec:hyperneat} covers the basics of HyperNEAT and how it is able to take advantage of game geometry. Next, Section \ref{sec:approach} presents our visual processing architecture and parallel computing framework. Results are presented in Section \ref{sec:results}, followed by future work and conclusions.

\section{Related Work}
\label{sec:background}
The study of general intelligence through the development of general game playing agents is not unique to this paper. Organizers in the field of General Game Playing (GGP) hold annual competitions for general game playing agents~\cite{genesereth05}. These agents are typically given a declarative description of an arbitrary game including a complete description of the game dynamics. They have no prior knowledge of this description and must formulate strategies to play this game on the fly. Unlike specialized game players, general game playing agents cannot rely on algorithms designed in advance for specific games. Successful agents typically incorporate artificial intelligence technologies such as knowledge representation, reasoning, learning, rational decision making and automatic theorem proving. 

While motivations are similar, the work reported in this paper differs from GGP in that Atari games are not formalized in such an abstract representation. Atari games convey state through a visual representation, and the dynamics of the game must first be learned before strategies can be formulated. Additionally, HyperNEAT-GGP currently only considers single player games as opposed to GGP players who compete against each other.

Another species of general game playing agents are found at the annual Ms Pacman competitions. Like HyperNEAT-GGP, Ms Pacman agents utilize actual screen representations~\cite{pacmancompetition} and must deal with non-determinism introduced by delays in processing the game screen along with false object detections. Successful entries in this competition have now far exceeded novice human players \cite{sigevolution2007}. HyperNEAT-GGP uses a an approach similar to the Ms Pacman agents in that both agents extract objects from the game screen. However, the object detection and game playing machinery used by HyperNEAT-GGP must be generally applicable enough to handle multiple games rather than specialized toward a single game such as Ms Pacman.

To the best of our knowledge, the first work on Atari game playing was an R-Max learning agent which employed an Object-oriented MDP representation~\cite{duik08}. Objects were detected in the game screen of the popular Atari game, \emph{Pitfall}. The agent was able to make it past the first screen. Subsequent work on learning in Atari games includes that of Naddaf~\cite{naddaf10}. Naddaf modified the popular Atari 2600 emulator, Stella, in order to allow it to be easily controlled by computer programs such as learning agents. Naddaf quantified the performance of several reinforcement learning and search agents over 50 different Atari games. Reinforcement learning agents included a gradient descent Sarsa$(\lambda)$ agent with linear function approximation, which could learn from feature vectors generated from either the game screen or the console RAM. Search tree agents include full tree search and UCT-based agents. Also motivated by the desire to create general game playing agents, Naddaf compiled experimental results of RL and search tree agents in over 50 Atari games. More than just a experimental benchmark, this work served as the inspiration for much of the visual processing described in Section \ref{sec:approach}.

Learning to play games based on overhead representations has been previously attempted by Verbancsics and Stanley~\cite{verbancsics10} who focused on the RoboCup Keepaway Soccer domain~\cite{stone01}. In this domain a number of \textit{keeper} agents must maneuver and pass a soccer ball so that it is not captured by one of the \textit{taker} agents. Verbancsics and Stanley encode the state of the game using an overhead representation of the objects on the playing field -- namely the keepers, takers, and the ball. In order to exploit the geometric regularities present in this overhead representation of the field, they employ HyperNeat. The learned policy is competitive with top learning algorithms for this task~\cite{stone05}. Additionally, the learned policy can be effectively transferred with no further learning to the same task at a higher resolution or a different number of players on the field. This successful transfer is a result of the indirect encoding of HyperNeat. HyperNEAT has also been successfully applied to other domains such as checkers~\cite{gauci08}, multi-agent predator prey~\cite{ambrosio08}, and quadruped locomotion~\cite{clune09}. 

While HyperNEAT-GGP is quite similar to Verbancsics and Stanley's approach, in many ways Atari games represent a more challenging learning target than RoboCup Keepaway. In Keepaway there are a fixed number of object classes such as takers, keepers, and the ball. On the other hand, Atari games may contain an arbitrary number of objects classes that interact with each other in unexpected ways, requiring HyperNEAT-GGP to be more general. Additionally, the dynamics in any given Atari game are highly variable, ranging from simple games in which the agent must reach the goal while avoiding cars to highly complex games in which the agent must shoot fish while attempting to rescue five swimmers, all before the oxygen in the player's submarine is depleted.

The next section describes the specifics of the Atari 2600 simulator and what makes it such an attractive platform for learning agents.

\section{Atari for Research}
\label{sec:atari}
The Atari 2600 video game console was released in October 1977. Notably, it was credited with creating game cartridges that decoupled game code from console hardware (previous devices all contained dedicated hardware with games already built in). Selling over 30 million consoles~\cite{atarihist}, Atari was considered wildly successful as an entertainment device. Today, while Atari is no longer at the forefront of entertainment, the console has good research potential for the three reasons:

First, the Atari console has a large collection of games. These games vary greatly from board games such as chess to action-exploration games like Pitfall to shooting games such as Asteroids and Space Invaders. Many games have support for a second player, opening the possibility of multi-agent learning. Having such a large number of games allows AI researchers to develop a single learning agent and then quickly and easily apply it to a large set of domains, facilitating the development of general game playing agents.

Second, a number of open-source Atari emulators exist, including projects such as Atari Learning Environment (ALE)\footnote{http://yavar.naddaf.name/ale/} that is designed specifically to accommodate learning agents. Furthermore, since the Atari 2600 CPU ran at 1.19 megahertz, modern emulators can run at high speeds of up to 2000 frames per second, making evaluation of agents and algorithms fast.

Third, the Atari state and action interface is simple enough for learning agents, but complex enough to control many different games. The state of an Atari game can be described relatively simply by its 2D graphics (containing between 8 and 256 colors depending on the color mode and a native resolution of 160x210), elementary sound effects, and 128 bytes of console RAM. The discrete action space for Atari consists of four possible directions of movement for the joystick and one button. Combinations of these two controls yield a total of 18 possible actions.

Having motivated the Atari as a suitable research platform for the development of general game playing agents, the next challenge is developing a capable HyperNeat-based learning agent.

\section{HyperNeat}
\label{sec:hyperneat}
This section reviews the fundamentals of the HyperNeat learning algorithm. HyperNEAT is an extension of the Neuro Evolution of Augmenting Topologies (NEAT) algorithm~\cite{stanley02}. NEAT evolves the topology and weights of an Artificial Neural Network (ANN) that is applied directly to the problem of interest. In contrast, HyperNEAT, introduced by Gauci and Stanley~\cite{gauci08}, evolves an \emph{indirect encoding} called a Compositional Pattern Producing Network (CPPN). The CPPN is then used to define the weights of an ANN that produces a solution for the problem. Furthermore, because the CPPN is aware of domain geometry, the ANNs it encodes implicitly contain knowledge about geometric relationships present in a given domain. This encoding allows HyperNEAT to take advantage of geometric regularities present in many board and 2D games, such as those in Atari 2600.

Specifically, HyperNEAT works in three stages:

\begin{enumerate}
\item The weights and topology of the CPPN are evolved. Internally a CPPN consists of functions such as Gaussians and sinusoids connected in a weighted topology (see Figure \ref{fig:cppn}).
\item The CPPN is used to determine the weights for every pair of \emph{(input,output)} nodes in a fully connected ANN.
\item With fully specified weights, the ANN is applied to the problem of interest. The performance of the ANN determines the fitness of the CPPN that generates it.
\end{enumerate}

\begin{figure}[htp]
\begin{center}
\includegraphics[width=\columnwidth]{figures/cppn}
\end{center}
\caption{HyperNEAT evolves the weights and topology of a CPPN (right). This CPPN is subsequently used to determine all of the weights between substrate nodes an ANN (left). Finally, the ANN is used to compute the solution to the desired problem. CPPNs are said to be geometrically aware because when they compute the weights of the associated ANN, they are given the geometric location of both the input and output node in the ANN.}
\label{fig:cppn}
\end{figure}

\section{Approach}
\label{sec:approach}
This section describes the components of HyperNEAT-GGP. The main points are the manner in which the raw Atari game screen is processed to form an overhead representation amenable to HyperNEAT, and the parallel framework used to speed up evolution.

\subsection{Visual Processing}
For nearly any machine learning problem, the question of how to encode the state space is of great importance. Similar to Verbancsics and Stanley's example, HyperNEAT-GGP uses an overhead object representation of the current game screen. Since the Atari provides only the raw pixels of the screen as input, a simple visual processing stack identifies objects and game entities without a priori knowledge of a specific game~\cite{naddaf10}. A graphical depiction of this stack is shown in Figure \ref{fig:visproc}.

\begin{figure*}[htp]
\begin{center}
\includegraphics[width=\textwidth]{figures/AtariArch}
\end{center}
\caption{Visual Processing Architecture applied to the game Freeway. Raw pixels from the game screen are displayed on the left. Next, contiguous pixels of the same color are merged into blobs. Objects are then extracted by merging adjacent blobs which exhibit constant velocity over the last two frames. Next, objects are clustered into object classes based on a pixel similarity score. Two main object classes are found -- cars facing left and cars facing right. This approach is similar to that of Naddaf~\cite{naddaf10}. Finally, self detection successfully identifies the chicken blob as the agent and colors it gray (circled in a red dashed line in rightmost screen).}
\label{fig:visproc}
\end{figure*}

Visual processing begins at the raw pixels of the game screen. Image segmentation groups adjacent raw pixels with similar colors into blobs. Next, blob merging occurs, outputting a set of current objects on screen. This process examines all of the recently discovered blobs and compares them with equivalent blobs in the last frame in order to compute a velocity for each blob. Blobs are matched between screens using pixel similarity. Velocity is computed by measuring the displacement of blob centroids. Once each blob is assigned a velocity, adjacent blobs with the same non-zero velocity are merged into objects. Objects that are too small or become stationary are thrown out. This check helps reduce the number of false positives in the object-detection process.

Finally, objects are clustered into object classes or prototypes. Specifically, the shape of each pair of objects is compared and if found to exceed a similarity threshold (97\% pixel match in these experiments), the objects are grouped into the same class. Pixel similarity between two objects is computed by comparing the presence of pixels relative to each object's bounding box. As Figure \ref{fig:visproc} indicates, different object classes are discovered for the cars at the bottom half of the screen, cars at the top half of the screen, and the chicken. 

To reduce the number of spurious prototypes, prototypes are passed to the next stage of the approach only when a number of instances of that prototype are seen within successive frames. In the example in Figure \ref{fig:visproc}, this check helps remove prototypes for objects created when the cars are at the edges of the screen. Prototypes failing this check are removed while those which pass are assigned a unique real number (see Section \ref{sec:interface} for more details).

Objects are assigned to the same class if their shape is relatively similar without taking color into consideration. This assumption has a potential drawback in certain games if different objects have similar shapes but different colors. 

\subsection{Self-Identification}
The self identification step is meant to identify the location of an on-screen entity that is being controlled by the agent. In the vast majority of Atari games, the player's actions affect the movement of some on-screen entity, here termed the \textit{self}. Knowledge of the location of the \textit{self} is crucial to selecting an action, as described below. HyperNEAT-GGP uses an approach based on information gain to identify a blob most likely to correspond to the \textit{self}. Pseudocode is given in Algorithm \ref{alg:idself}.

\begin{algorithm}
\caption{Identify Self}
\label{alg:idself}
\begin{algorithmic}[1]
  \STATE $actions \leftarrow $ set of actions applicable to this game
  \STATE $current\_blobs \leftarrow $ set of blobs in the current game frame
  \STATE $ActionHistory \leftarrow \{a_0...a_n\}$ %\COMMENT{Actions at time 0...n}
  \FOR{blob $b \in current\_blobs$}
  \STATE $vHist_b \leftarrow \{v_0...v_n\}$ %\COMMENT{Velocity of blob $b$ at each timestep in the history}
  %\STATE \COMMENT{Information entropy of $b$'s velocity history}
  \STATE $H_b \leftarrow H(vHist_b)$ 
  \FOR{action $a \in actions$}
  %\STATE \COMMENT{set of $b$'s velocities for timesteps in which action $a$ was taken}
  \STATE $vHist_{(b|a)} \leftarrow [vHist_b[t] ~\forall_t: ActionHist[t-1] == a]$ 
  \STATE $H_{(b|a)} \leftarrow H(vHist_{(b|a)})$ %\COMMENT{Information entropy of $b$'s velocity history given action $a$ was taken}
  \ENDFOR
  \STATE $InfoGain_b \leftarrow H_b - sum_{a \in actions}(p_a * H_{(b|a)})$ %\COMMENT{$p_a$ is probability of action $a$ based on observed frequency} 
  \ENDFOR
  \RETURN $arg\_max_{b \in current\_blobs}(InfoGain_b)$ %\COMMENT{Return blob with max information gain}
\end{algorithmic}
\end{algorithm}

At the high level, this approach makes certain assumptions about the self entity. First, it is assumed that the self blob will move similarly whenever the same action is performed. That is, whenever an action, say Joystick Up, is taken, the resulting velocity of the self blob should have a similar value (e.g. blob.y\_velocity = -1). 

As input, in lines 1-3, the algorithm has access to the set of possible joystick and button actions applicable to the current game (this is typically a subset of the 18 possible actions present on the Atari console), a list of blobs detected in the current frame, and a history of the actions taken by the agent. Additionally, line 5 assumes access to the $(x,y)$ velocity history $vHist$ of every blob. Next, Algorithm \ref{alg:idself} computes the entropy of blob $b$'s velocity history. Entropy is calculated using the standard formulation: $H(X) = -\sum_{i=1}^n{p(x_i)*lg(p(x_i))}$. Taking entropy over a velocity history involves computing the distribution over blob $b$'s velocity values. This computation is done using the empirically observed frequencies of each observed $(x,y)$ velocity value in $b$'s velocity history. A blob with highly random movement will exhibit a high information entropy over its full velocity history while a blob with highly regular movement will yield low entropy.

Having computed the information entropy over $b$'s full velocity history, Algorithm \ref{alg:idself} next examines each action individually and, in line 8, create $b$'s selective velocity history $vHist_{(b|a)}$. The selective velocity history simply filters the full velocity history by including only velocities that were observed in frames after which action $a$ was taken. For example, if $a = $ joystick left, then $vHist_{(b|a)}$ would only contain resultant velocities for frames in which $a$ was the action selected. In line 9, entropy is computed over $b$'s selective velocity history. This selective entropy $H_{(b|a)}$ should be low if a given action reliably causes the blob to move in a certain direction.

\begin{figure*}
  \centering
  \subfloat[Object Classes]{\label{fig:classes}\includegraphics[width=0.196\textwidth]{figures/objClasses.png}}\hspace{.1in}
  \subfloat[Grid Overlay]{\label{fig:grid}\includegraphics[width=0.2\textwidth]{figures/grid-overlay.png}}\hspace{.1in}
  \subfloat[ANN Input]{\label{fig:anninput}\includegraphics[width=0.2\textwidth]{figures/ann-input.png}}\hspace{.1in}
  \subfloat[ANN Output]{\label{fig:annoutput}\includegraphics[width=0.2\textwidth]{figures/action-selection.png}}
 \caption{The interface between visual processing framework and the HyperNEAT ANN. Classes of objects are discretized into a grid whose cells are fed to the input nodes of the ANN via a map from object classes to real numbers. After running the ANN, activations of the output layer in cells adjacent to the detected self are used to select which action the agent should take. In this case, the agent would move up since that adjacent cell has the highest activation.}
 \label{fig:interface}
\end{figure*}

Finally, information gain is computed in line 11 by subtracting a frequency weighted sum of a blob's selective velocity entropies from the blob's full velocity entropy. If each of the selective entropies is low, as should be the case for the self blob, a high information gain results. In line 13, the algorithm concludes by returning the blob with maximum information gain.

While Algorithm \ref{alg:idself} is generally successful in identifying the self blob, sometimes game dynamics break the assumption that actions result in similar velocities. For example, in the Freeway game, after colliding with a car, control is taken from the player and the chicken inadvertently is moved down for several frames regardless of which actions the agent is executing. This temporary lack of control results in irregular selective velocity histories and temporarily poorer identification of the self. However, in some sense the algorithm is correct in losing confidence in an object over which it no longer has control.

\subsection{Atari-HyperNEAT Interface}
\label{sec:interface}
After extracting object classes as well as the location of the self from the raw game screen, this information needs to be sent to HyperNEAT. As discussed in Section \ref{sec:hyperneat}, HyperNEAT evolves a CPPN that encodes an ANN. In this section assumes access to a fully connected 2-layer ANN whose weights have been specified by the CPPN. At a high level, information from the game screen needs to be translated to activations of nodes in the input layer of the ANN. Then, after the network has been run in the standard feed-forward fashion, the activation of nodes on the output layer must be interpreted in order to select an action.

Figure \ref{fig:interface} shows an example of how object classes are given as input to the substrate layer of the ANN. Since the ANN input nodes can only take real-valued activations, each class of objects must be mapped to a real number. Thus a mapping from object classes to real values is maintained. Upon discovery of new object classes, real values are incrementally added to the map in a non-decreasing fashion. Next, since the ANN contains a $16x21$ grid of input nodes, the raw game screen must also be discretized. Figure \ref{fig:grid} shows an example discretization. Following this step, each cell that contains an object is fed as input to the ANN with a real value corresponding to that object class's real value. Cells devoid of objects are given input activations of zero. The ANN is run in a standard feed-forward manner, producing activations of the nodes in the $16x21$ output layer (Figure \ref{fig:annoutput}). Action selection involves locating the cell corresponding to the detected self object (colored blue in Figure \ref{fig:annoutput}). The activation of this cell as well as the activations of the four adjacent cells (shown with red arrows) are compared and the action corresponding to the arrow in the highest of these five cells is returned (or no-op if the self square has the highest value). This method of action selection supports up to five possible actions. Future work involves extending this framework to support more complex actions such a button presses and combinations of joystick and button-press actions.

\subsection{Parallel Implementation}
\label{sec:condor}
The visual processing and self-identification components of the code require a significant amount of processing time each frame. Depending on the game being played, evaluating a single individual can take anywhere between a few seconds to five minutes. Initial experiments indicated that HyperNEAT would need to run for \textit{at least} 250 generations with 100 individuals per generation to obtain representative results. These parameters make it infeasible to run the entire computation on a single machine in a reasonable amount of time. To speed up evolution, the parallel framework exploits the parallel nature of this approach by evaluating each individual on a separate machine. This framework makes use of the Condor distributed processing cluster \cite{thain2005distributed} to run a massive number of Atari simulations in parallel. A similar framework has been used in the past to speed up the optimization of robot soccer agents in simulation using CMA-ES\cite{Urieli+MKBS:2010}. The HyperNEAT implementation of Naddaf~\cite{verbancsics10} was also modified to run in parallel on the Condor cluster.

% To this end, we exploit the embarassingly parallel nature of the evaluating a number of individuals in a single generation. We use the Depending on the game being played in the Atari simulator, a single evaluation of an individual can last from anything between a few seconds to 2 or 3 minutes. While keeping the evolution parameters to as low as 100 individuals and 300 generations, it is still not possible to obtain results in sufficient time on a single machine. To mitigate this problem, we exploit the embarassingly parallel nature of evaluating individuals. We have modifed the HyperNEAT codebase to run on the Condor cluster, by adapting the framework developed by Yinon Bentor and Patrick MacAlpine.\footnote{This framework was initially developed in a previous iteration of the CS394N class. Since then it has been used by the AustinVilla Robot Soccer team for optimizing policy parameters for various soccer related tasks using evolutionary strategies such as CMA-ES.}

% \begin{figure*}[ht]
% \begin{center}
% \includegraphics[width=\textwidth]{figures/CondorHyperNEAT}
% \end{center}
% \caption{Procedure for using HyperNEAT on the condor framework. An initial Master process is responsible for running NEAT and prodcuding successive generations. Condor jobs are initialized with each job being provided the population file and the id of th individual it is responsible for evaluating. Each job starts an instance of the Atari emulator and proceeds to evaluate the individual by playing the game using the policy defined by that individual. A fitness value is calculated, corresponding to the score obtained in the game and written out to file. This is read back by the Master process which then proceeds to use the fitness values to produce the next generation.}
% \label{fig:condor}
% \end{figure*}

\begin{figure}[ht]
\begin{center}
\includegraphics[width=\columnwidth]{figures/condor-hyperneat-small.png}
\end{center}
\caption{Parallel evaluations on the Condor framework.}
\label{fig:condor}
\end{figure}

Figure \ref{fig:condor} details how parallel evaluations are performed using the Condor cluster. Once a generation is produced, all the individuals of that generation are evaluated on separate machines in parallel. After all evaluations have been completed, the fitness values are gathered and a new population is produced. This process is then repeated for each successive generation. 

One practical problem while using the framework was that certain fitness evaluations do not complete and are assigned the minimum score. As a result, some individuals may be lost before maturing enough to improve other members of the population. Although such a situation is not ideal, we believe the evolutionary process is robust enough to compensate. This hypothesis is corroborated by the steady increase of the average champion fitness in the experiments (see Fig. \ref{fig:asterix-curve}). 

\section{Experimental Setup}
\label{sec:exprsetup}
The experimental setup was the same for both Freeway and Asterix: HyperNEAT-GGP was run for 250 generations, with 100 individuals in each generation and a substrate resolution of 16x21. All individuals were evaluated on the Atari simulator using the same random seed. Results are averaged across multiple runs of the HyperNEAT-GGP evolution and compared with previous results obtained using reinforcement learning \cite{naddaf10}. 

\section{Results}
\label{sec:results}
This section provides results obtained from applying HyperNEAT-GGP on two Atari games, \textit{Freeway} and \textit{Asterix}. Table 1 presents the fitness values of the best individual averaged across all the runs, as well as the overall best individual found. BASS, DISCO, and RAM are gradient descent Sarsa$(\lambda)$ agents with linear function approximation. These methods use state representations corresponding to the sub-sampled screen image, objects on screen, and console RAM respectively. Results are also compared against an agent taking random actions each step~\cite{naddaf10}. 

\subsection {Freeway}

Freeway is an Atari game similar to the arcade game \textit{Frogger}. The player controls a chicken with the objective of reaching the top of the highway while avoiding cars (Fig. \ref{fig:visproc}). The player receives a score of 1 if the chicken reaches the top of the highway, at which point the chicken moves back to the bottom of the screen. If the chicken gets hit by a car, it gets pushed down by a fixed small distance. The game ends after two minutes of game-play. There are three possible actions that the player may use in Freeway: \textit{Up}, \textit{Down}, and \textit{No-Action}.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=\columnwidth]{figures/freeway-results.png}
\end{center}
\caption{HyperNEAT learning performance on the Freeway game. Average fitness of the population along with the champion fitness for each generation are displayed. Error bars represent standard deviation. The fitness of an individual corresponds exactly to its game score. As the figure shows, effective Freeway policies are found in early generations.}
\label{fig:freeway-curve}
\end{figure}

\commentm{Change graph style so that we can differentiate between the two lines in b/w.}

% \ref{tab:results-table} points incorrectly to 6?
Average fitness of the population and the champion fitness throughout the learning process can be seen in Fig. \ref{fig:freeway-curve}. Results were averaged across five runs of HyperNEAT evolution. Table 1 contrasts HyperNEAT-GGP with previous results. The most striking observation from these results is that reinforcement learning is unable to find a solution to the problem, whereas even the first generation in HyperNEAT produces a champion with a very high fitness (starting average champion fitness = 26.4). Reinforcement learning has to perform a large number of exploratory actions before it can reach the goal state at the top, and succumbs to the highly delayed nature of the reward in this game. On the other hand, all individuals in HyperNEAT are produced by randomly initialized CPPNs. Due to the nature of CPPNs, at least a few individuals have the propensity to always take the \textit{Up} action and obtain a large fitness at the outset. Evolution helps individuals to learn quickly to avoid cars over the next 50 generations, which can be seen by the slight increase in champion and average fitness.

\subsection {Asterix}
In Asterix, the player controls a unit called Asterix with the objective of collecting magic potions and avoiding \textit{lyres}. Both the magic potions and the lyres move across the screen around Asterix. Each time Asterix collects a magic potion, the player receives a score of 50. The game is over when the Asterix touches a lyre. Asterix can take five possible actions: \textit{Up}, \textit{Down}, \textit{Left}, \textit{Right}, and \textit{No-Action}.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=\columnwidth]{figures/asterix-results.png}
\end{center}
\caption{HyperNEAT learning performance on the Asterix game. The average fitness of the population along with the champion at each generation in the Asterix game. Error bars represent standard deviation. Fitness of an individual corresponds exactly to their game score. As the figure shows, policies are continually improved throughout the course of the 250 generations.}
\label{fig:asterix-curve}
\end{figure}

Average fitness of the population and the champion fitness throughout the learning process can be seen in Fig. \ref{fig:asterix-curve}. Results for Asterix were averaged across 10 runs of HyperNEAT evolution. Table 1 contrasts our approach with previous results. The results for Asterix are qualitatively different from Freeway in a number of ways:
\begin{itemize}
\item
Random exploration obtains non-zero reward in Asterix as Asterix inadvertently collects magic potions. Reinforcement Learning approaches can bootstrap from this information and learn to become statistically better than random.
\item
Champions from HyperNEAT evolution start at close to random performance (starting average champion fitness = 190) and improve their performance to the same level as that of reinforcement learning approaches within 50 generations. 
\item
The learning process steadily improves the fitness through the entirety of 250 generations. This steady improvement demonstrates the power of using CPPNs at representing good policies for this game.
\end{itemize}

%Results from the Asterix game are summarized in Figure \ref{fig:asterix-curve} and Table 1. Unlike the Freeway game, the champion fitness curve shows significant improvement accross successive generations. Secondly, the champion fitness reaches a high of 750 before dropping down to around the 550 mark. We suspect that our Condor implementaion is responsible, and that the champion was lost to an incomplete Condor job (see Section \ref{sec:condor}). We hope to rectify this issue in the future by making our framework more robust. In contrast to the Freeway game, RL methods perform decently well in this domain. Initial exploration probably causes the agent to learn to stay away from the sprites much faster in this case. However HyperNEAT still outperforms RL based methods. On the other hand, HyperNEAT does not perform as well as a human player. We attribute this to the higher complexity of the game, as well as multiple object classes (which is something we are still working on). 

\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
~ & \textbf{Freeway} & \textbf{Asterix} \\ \hline
\textbf{BASS} & 0 & 402 \\ \hline
\textbf{DISCO} & 0 & 301 \\ \hline
\textbf{RAM} & 0 & 545 \\ \hline
\textbf{Random} & 0 & 156 \\ \hline
\textbf{HyperNEAT-GGP(Average)} & 28.4 & 725 \\ \hline
\textbf{HyperNEAT-GGP(Best)} & 29 & 1000 \\ 
\hline
\end{tabular}
\end{center}
\label{tab:results-table}
\caption{Game scores obtained in the Freeway and Asterix games. HyperNEAT-GGP substantially outperforms related RL techniques on both Freeway and Asterix.}
\end{table}

Results show excellent performance by HyperNEAT on the Asterix and Freeway games. However, to extend HyperNEAT to play arbitrary games in the Atari simulator, some future work is required.

\section{Future Work}
\label{sec:futurework}
The most pressing direction for future work is to extend HyperNEAT-GGP to a larger set of games. There are two main challenges: large numbers of possible actions and many different object classes.

In Freeway and Asterix, like in Robocup Keepaway, there are relatively few classes of objects that matter: cars and the chicken for Freeway, potions and lyres for Asterix, and takers and keepers for Keepaway. With a limited number of object classes it is easy to map from objects to substrate values. For example, in Keepaway, keepers can be assigned values of 1 and takers values of -1. Having such few values allows HyperNEAT to easily differentiate between classes of objects and exhibit appropriate behaviors for each, such as avoiding lyres and collecting potions.

It is more difficult to differentiate between object classes as the number of classes increases and crowds the map of object class to substrate values. In the worst case, if two opposite classes of objects (e.g. keepers and takers) get mapped to the same real value\commentp{this needs to be rewritten}, HyperNEAT cannot distinguish between them and formulate appropriate strategies for dealing with each class.

One possible solution is to create input substrates, with one substrate for each class of objects, as seen in Figure \ref{fig:possiblearch}. Each new object class is given its own substrate layer which it exclusively populates. At run-time, each of the substrate layers is connected to the processing layer and run in a feed-forward manner. To accommodate multiple substrates, an additional input needs to be added to the CPPN to specify which substrate is currently active. In this way, HyperNEAT would, with some additional computational cost, be able to encode an entirely different policy for an arbitrary number of object classes. 

The second area of future work involves developing a better way to handle a large number of actions. In games like Freeway in which there are only a few actions (up, down, no-op), it is possible to choose which action to take by examining values of the output nodes adjacent to the self node (as described in Section \ref{sec:interface}). This issue becomes more complicated when other actions such as button presses are involved. For example, which node's value should be examined to decide if the button press action should be taken? To address this issue, another output layer above the processing layer can be included. This additional output layer would have a single node for each of the possible actions in the game. Action selection would simply reduce to finding the action node with the highest value. If additional actions need to be introduced, more action nodes can be created in the output layer. The top portion of Figure \ref{fig:possiblearch} depicts this alternative architecture.

\begin{figure}[htp]
\begin{center}
\includegraphics[width=\columnwidth]{figures/multiple-substrate.png}
\end{center}
\caption{Alternative architecture for handling variable numbers of object classes and actions. New object classes are assigned individual substrate layers. Additionally, above the processing level exists a node for each possible action. At run-time activations are propagated up from all $N$ substrate layers to the processing and action layers. Action selection involves picking which action node has the highest activation.}
\label{fig:possiblearch}
\end{figure}

\section{Conclusion}
\label{sec:conclusion}
This paper introduces HyperNEAT-GGP, a HyperNEAT-based general Atari game playing agent. Many Atari games contain geometric regularities in the two-dimensional space of the game screen. This structure allows HyperNEAT to quickly learn effective policies. To reduce the complexity of learning from the raw game screen, HyperNEAT-GGP employs a game-independent visual processing hierarchy designed to identify classes of objects as well as the entity that the player controls on the game screen. Identified objects are provided as input to HyperNEAT. Due to the computational overhead of visual processing applied to each game screen, a parallel architecture is used to evaluate multiple individuals simultaneously. Results were presented for two Atari games, \textit{Freeway} and \textit{Asterix}. In both cases, HyperNEAT-GGP was shown to outperform previous reinforcement learning benchmarks~\cite{naddaf10}. While no single Atari game, if studied in isolation and given extensive feature engineering, likely poses too great a challenge for modern AI techniques, the full collection of over 900 Atari games presents a daunting task for a single learning agent. HyperNEAT-GGP represents a first step towards the ambitious goal of creating an agent capable of learning and seamlessly transitioning between many different tasks.

%\end{document}  % This is where a 'short' article might terminate

%ACKNOWLEDGMENTS are optional
%% \section{Acknowledgments}
%% This section is optional; it is a location for you
%% to acknowledge grants, funding, editing assistance and
%% what have you.  In the present case, for example, the
%% authors would like to thank Gerald Murray of ACM for
%% his help in codifying this \textit{Author's Guide}
%% and the \textbf{.cls} and \textbf{.tex} files that it describes.

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{hyperneat}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references

\end{document}
