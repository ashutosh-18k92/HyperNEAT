% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009

\documentclass{sig-alternate}
%\usepackage[pdftex]{graphicx}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[bf]{caption}
\usepackage{subfig}

\DeclareCaptionType{copyrightbox}
\renewcommand{\algorithmiccomment}[1]{//#1}

\long\def\commentp#1{{\bf **P: #1**}}
\long\def\commentm#1{{\bf **M: #1**}}

\begin{document}
\conferenceinfo{GECCO'12,} {July 7--11, 2012, Philadelphia, Pennsylvania, USA.}
\CopyrightYear{2012}
\crdata{978-1-4503-1177-9/12/07}
\clubpenalty=10000
\widowpenalty = 10000
\title{HyperNEAT-GGP: A HyperNEAT-based \\Atari General Game Player}
%\subtitle{[Extended Abstract]
%% \titlenote{A full version of this paper is available as
%% \textit{Author's Guide to Preparing ACM SIG Proceedings Using
%% \LaTeX$2_\epsilon$\ and BibTeX} at
%% \texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

%% \numberofauthors{8} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
%% \author{
%% % You can go ahead and credit any number of authors here,
%% % e.g. one 'row of three' or two rows (consisting of one row of three
%% % and a second row of one, two or three).
%% %
%% % The command \alignauthor (no curly braces needed) should
%% % precede each author name, affiliation/snail-mail address and
%% % e-mail address. Additionally, tag each line of
%% % affiliation/address with \affaddr, and tag the
%% % e-mail address with \email.
%% %
%% % 1st. author
%% \alignauthor
%% Ben Trovato\titlenote{Dr.~Trovato insisted his name be first.}\\
%%        \affaddr{Institute for Clarity in Documentation}\\
%%        \affaddr{1932 Wallamaloo Lane}\\
%%        \affaddr{Wallamaloo, New Zealand}\\
%%        \email{trovato@corporation.com}
%% % 2nd. author
%% \alignauthor
%% G.K.M. Tobin\titlenote{The secretary disavows
%% any knowledge of this author's actions.}\\
%%        \affaddr{Institute for Clarity in Documentation}\\
%%        \affaddr{P.O. Box 1212}\\
%%        \affaddr{Dublin, Ohio 43017-6221}\\
%%        \email{webmaster@marysville-ohio.com}
%% % 3rd. author
%% \alignauthor Lars Th{\o}rv{\"a}ld\titlenote{This author is the
%% one who did all the really hard work.}\\
%%        \affaddr{The Th{\o}rv{\"a}ld Group}\\
%%        \affaddr{1 Th{\o}rv{\"a}ld Circle}\\
%%        \affaddr{Hekla, Iceland}\\
%%        \email{larst@affiliation.org}
%% \and  % use '\and' if you need 'another row' of author names
%% % 4th. author
%% \alignauthor Lawrence P. Leipuner\\
%%        \affaddr{Brookhaven Laboratories}\\
%%        \affaddr{Brookhaven National Lab}\\
%%        \affaddr{P.O. Box 5000}\\
%%        \email{lleipuner@researchlabs.org}
%% % 5th. author
%% \alignauthor Sean Fogarty\\
%%        \affaddr{NASA Ames Research Center}\\
%%        \affaddr{Moffett Field}\\
%%        \affaddr{California 94035}\\
%%        \email{fogartys@amesres.org}
%% % 6th. author
%% \alignauthor Charles Palmer\\
%%        \affaddr{Palmer Research Laboratories}\\
%%        \affaddr{8600 Datapoint Drive}\\
%%        \affaddr{San Antonio, Texas 78229}\\
%%        \email{cpalmer@prl.com}
%% }
%% % There's nothing stopping you putting the seventh, eighth, etc.
%% % author on the opening page (as the 'third row') but we ask,
%% % for aesthetic reasons that you place these 'additional authors'
%% % in the \additional authors block, viz.
%% \additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
%% email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
%% (The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
%% \date{30 July 1999}
%% % Just remember to make sure that the TOTAL number of authors
%% % is the number that will appear on the first page PLUS the
%% % number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
This paper considers the challenge of enabling agents to learn with as little domain-specific knowledge as possible. The main contribution is HyperNEAT-GGP, a HyperNEAT based approach to playing general Atari games using a visual representation of the game screen. By leveraging the geometric regularities present in the Atari game screen, HyperNEAT effectively evolves policies for playing two different Atari games, Asterix and Freeway. Results show that HyperNEAT-GGP outperforms existing benchmarks on these games. HyperNEAT-GGP represents a step towards the ambitious goal of creating an agent capable of learning and seamlessly transitioning between many different tasks.
\end{abstract}

% A category with the (minimum) three required fields
%% \category{H.4}{Information Systems Applications}{Miscellaneous}
%% %A category including the fourth, optional field follows...
%% \category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

%% \terms{Theory}

%% \keywords{ACM proceedings, \LaTeX, text tagging} % NOT required for Proceedings

\section{Introduction}
A major challenge for AI is to develop agents that can learn and perform many different tasks. To this end, this paper aims at developing a learning agent capable of playing a large number of games with as little domain specific knowledge as possible. Famous game playing AI systems such as Deep Blue for chess, Watson for Jeopardy, and TD-Gammon for backgammon~\cite{tesauro_94} all demonstrate that with enough manpower and ingenuity it is possible to tackle AI challenges that may have previously seemed insurmountable. Unlike these game intelligences which were created and tuned specifically for a single task, the game playing agent described herein must be general enough to tackle many different Atari 2600 games. This requires general intelligence to be built into the agent itself rather than just present in the programmer of the agent.

This work focuses on learning to play Atari 2600 games, a middle ground between classic board games and newer, graphically intensive video games. The Atari 2600 is strictly at least as complicated as most board games (since games such as chess and checkers have been created for the Atari) yet contains none of the 3-D graphics of newer video games. Like traditional board games, the Atari provides opportunities for agents to benefit from a solid understanding of the game's dynamics and allows for careful planning. At the same time, like modern video games, the Atari incorporates simple visual representations that need to be processed and interpreted. Dynamics of Atari games vary wildly from Checkers to Space Invaders, necessitating the use of general learning algorithms.

Despite the variability of game dynamics, all Atari games share a standard interface designed for humans to interact with and enjoy. Game state is conveyed to the player through a 2D game screen, and in response, the player controls game elements by manipulating a joystick and pressing a single button. This standard interface, combined with the large number of available games, makes Atari a convenient platform for AI researchers. 

\begin{figure}[t]
  \centering
  \subfloat{\label{fig:asterix}\includegraphics[width=0.205\textwidth]{figures/asterix}}\hspace{.1in}
  \subfloat{\label{fig:freeway}\includegraphics[width=0.2\textwidth]{figures/freeway}}\hspace{.1in}
 \caption{Asterix and Freeway, two of the many games available for the Atari 2600.}
 \label{fig:samplegames}
\end{figure}

This paper presents HyperNEAT-GGP, an agent which uses an evolutionary algorithm called Hypercube-based NeuroEvolution of Augmenting Topologies (HyperNEAT)~\cite{gauci08}. Unlike most other approaches, HyperNEAT is capable of exploiting geometric regularities present in the 2D game screen in order to evolve highly effective game playing policies. This paper applies HyperNEAT-GGP to the two Atari games shown in Figure \ref{fig:samplegames}, Freeway and Asterix.

Freeway and Asterix were selected because their game dynamics were sufficiently different from each other, yet both are amenable to HyperNEAT and have been studied in the past \cite{naddaf10}. Briefly, the objectives of each game are as follows:

In Freeway the player controls a chicken as it crosses a ten lane highway filled with traffic in an effort to ``get to the other side.'' The chicken is allowed to move only up, down, or remain in place. Colliding with a car results in the chicken being thrown a distance towards the bottom of the screen. Each time a chicken reaches the top of the screen, the player is rewarded a point and the chicken respawns at the bottom of the screen. Gameplay continues for two minutes and sixteen seconds. 

In Asterix the player controls a unit called Asterix in his quest to collect as many objects as possible while avoiding deadly lyres. Asterix can move in any of the four cardinal directions and receives 50 score whenever he collects a magic potion. Asterix loses a life when he touches a lyre and when Asterix has exhausted his supply of lives the game ends. Lyres and collectible objects spawn along the edges of the screen and move horizontally. Objects gain speed as the game progresses, necessitating quick reflexes and decisions.

The next section contains related work. Section \ref{sec:atari} discusses the merits of the Atari 2600 console as a research platform. Section \ref{sec:hyperneat} covers the basics of HyperNEAT and how it is able to take advantage of game geometry. Next, Section \ref{sec:approach} presents our visual processing architecture and parallel computing framework. Experiments are presented in Section \ref{sec:experiments}, followed by future work and conclusions.


\section{Related Work}
\label{sec:background}
The study of general intelligence through the development of general game playing agents is not unique to this paper. Organizers in the field of General Game Playing (GGP) hold annual competitions for general game playing agents~\cite{genesereth05}. These agents are typically given a declarative description of an arbitrary game including a complete description of the game dynamics. They have no prior knowledge of this description and must formulate strategies to play this game on the fly. Unlike specialized game players, general game playing agents cannot rely on algorithms designed in advance for specific games. Successful agents typically incorporate artificial intelligence technologies such as knowledge representation, reasoning, learning, rational decision making and automatic theorem proving. 

While motivations are similar, the work reported in this paper differs from GGP in that Atari games are not formalized in such an abstract representation. Atari games convey state through a visual representation, and the dynamics of the game must first be learned before strategies can be formulated. Additionally, HyperNEAT-GGP currently only considers single player games as opposed to GGP players who compete against each other.

Another class of game playing agents is found at the annual Ms. Pac-Man competition. Like HyperNEAT-GGP, Ms. Pac-Man agents utilize actual screen representations~\cite{pacmancompetition} and must deal with non-determinism introduced by delays in processing the game screen. Successful entries in this competition have now far exceeded novice human players \cite{sigevolution2007}. HyperNEAT-GGP uses an approach similar to the Ms Pac-Man agents in that both agents extract objects from the game screen. However, the object detection and game playing machinery used by HyperNEAT-GGP must be generally applicable enough to handle multiple games rather than specialized toward a single game such as Ms Pac-Man.

Moving into 3D environments, backpropagation and Lamarckian neuroevolution were used to train a neural network visual controller for agents in the Quake II environment \cite{parker09}. In order to process the 3D game screen, the agents used an artificial retina consisting of 28 grayscale blocks arranged into a grid spanning the width of the game screen. Like the human retina, more blocks were clustered near the center of the visual field to allow the agent to better recognize if the target was directly ahead. The resulting agent was able to navigate a room with a large central pillar, seek out an enemy, and shoot it. 

To the best of our knowledge, the first work on Atari game playing was an R-Max learning agent which employed an Object-oriented MDP representation~\cite{duik08}. Objects were detected in the game screen of the popular Atari game, \emph{Pitfall}. The agent was able to make it past the first screen. Subsequent work on learning in Atari games includes that of Naddaf~\cite{naddaf10}. Naddaf modified the popular Atari 2600 emulator, Stella, in order to allow it to be easily controlled by computer programs such as learning agents. Naddaf quantified the performance of several reinforcement learning and search agents over 50 different Atari games. Reinforcement learning agents included a gradient descent Sarsa$(\lambda)$ agent with linear function approximation, capable of learning from feature vectors generated from either the game screen or the console RAM. Search tree agents include full tree search and UCT-based agents. Also motivated by the desire to create general game playing agents, Naddaf compiled experimental results of RL and search tree agents in over 50 Atari games. More than just a experimental benchmark, this work served as the inspiration for much of the visual processing described in Section \ref{sec:approach}.

Learning to play games based on overhead representations has been previously attempted by Verbancsics and Stanley~\cite{verbancsics10} who focused on the RoboCup Keepaway Soccer domain~\cite{stone01}. In this domain a number of \textit{keeper} agents must maneuver and pass a soccer ball so that it is not captured by one of the \textit{taker} agents. Verbancsics and Stanley encode the state of the game using an overhead representation of the objects on the playing field -- namely the keepers, takers, and the ball. HyperNEAT is used to exploit the geometric regularities present in this overhead representation of the field.  The learned policy is competitive with top learning algorithms for this task~\cite{stone05}. Additionally, the learned policy can be effectively transferred with no further learning to the same task at a higher resolution or a different number of players on the field. This successful transfer is a result of the indirect encoding of HyperNEAT. HyperNEAT has also been successfully applied to other domains such as checkers~\cite{gauci08}, multi-agent predator prey~\cite{ambrosio08}, and quadruped locomotion~\cite{clune09}. 

While HyperNEAT-GGP is quite similar to Verbancsics and Stanley's approach, in many ways Atari games represent a more challenging learning target than RoboCup Keepaway. In Keepaway there are a fixed number of object classes such as takers, keepers, and the ball. On the other hand, Atari games may contain an arbitrary number of object classes that interact with each other in unexpected ways, requiring HyperNEAT-GGP to be more general. Additionally, the dynamics in any given Atari game are highly variable, ranging from simple games in which the agent must reach the goal while avoiding cars to highly complex games in which the agent must shoot fish while attempting to rescue five swimmers, all before the oxygen in the player's submarine is depleted. Though this paper only demonstrates results on two of the many Atari games, the successful results represent an important step towards the long-term goal of testing the approach on, and refining it to generalize to, the full range of Atari games.

\section{Atari for Research}
\label{sec:atari}
The Atari 2600 video game console was released in October 1977. It was the first console to create game cartridges that decoupled game code from console hardware (previous devices all contained dedicated hardware with games already built in). Selling over 30 million consoles~\cite{atarihist}, Atari was considered wildly successful as an entertainment device. Today, while Atari is no longer at the forefront of entertainment, the console has good research potential for three reasons:

First, the Atari console has a large collection of games. These games vary greatly from board games such as chess to action-exploration games like Pitfall to shooting games such as Asteroids and Space Invaders. Many games have support for a second player, opening the possibility of multi-agent learning. Having such a large number of games allows AI researchers to develop a single learning agent and then quickly and easily apply it to a large set of domains, facilitating the development of general game playing agents.

Second, a number of open-source Atari emulators exist, including projects such as Atari Learning Environment (ALE)\footnote{http://yavar.naddaf.name/ale/} that are designed specifically to accommodate learning agents. Furthermore, since the Atari 2600 CPU ran at 1.19 megahertz, modern emulators can run at high speeds of up to 2000 frames per second, expediting the evaluation of agents and algorithms.

Third, the Atari state and action interface is simple enough for learning agents, but complex enough to control many different games. The state of an Atari game can be described relatively simply by its 2D graphics (containing between 8 and 256 colors depending on the color mode and a native resolution of $160\times 210$), elementary sound effects, and 128 bytes of console RAM. The discrete action space for Atari consists of eight directions of movement for the joystick (up, down, left, right, up\&left, up\&right, etc) as well as a single button. This button can be pressed alone or simultaneously with any of the joystick movements. Including \textit{NO-OP}, this yields a total of 18 possible actions.

Having motivated the Atari as a suitable research platform for the development of general game playing agents, the next challenge is developing a capable HyperNEAT-based learning agent.

\section{HyperNEAT}
\label{sec:hyperneat}
This section reviews the fundamentals of the HyperNEAT learning algorithm. HyperNEAT is an extension of the Neuro Evolution of Augmenting Topologies (NEAT) algorithm~\cite{stanley02}. NEAT evolves the topology and weights of an Artificial Neural Network (ANN) that is applied directly to the problem of interest. In contrast, HyperNEAT, introduced by Gauci and Stanley~\cite{gauci08}, evolves an \emph{indirect encoding} called a Compositional Pattern Producing Network (CPPN). The CPPN is then used to define the weights of an ANN that produces a solution for the problem. Furthermore, because the CPPN is aware of domain geometry, the ANN it encodes implicitly contains knowledge about geometric relationships present in a given domain. In comparison to standard NEAT, HyperNEAT's encoding allows it to take advantage of geometric regularities present in many board and 2D games, such as those in Atari 2600. 

Specifically, HyperNEAT works in four stages:

\begin{enumerate}
\item The weights and topology of the CPPN are evolved. Internally a CPPN consists of functions such as Gaussians and sinusoids connected in a weighted topology (see Figure \ref{fig:cppn}).
\item The CPPN is used to determine the weights for every pair of \emph{(input,output)} nodes in a fully connected ANN.
\item With fully specified weights, the ANN is applied to the problem of interest. The performance of the ANN determines the fitness of the CPPN that generates it.
\item Based on fitness scores, the population of CPPNs is maintained, evaluated, and evolved via NEAT.
\end{enumerate}

\begin{figure}[htp]
\begin{center}
\includegraphics[width=\columnwidth]{figures/cppn}
\end{center}
\caption{HyperNEAT evolves the weights and topology of a CPPN (right). This CPPN is subsequently used to determine all of the weights between substrate nodes in the ANN (left). Finally, the ANN is used to compute the solution to the desired problem. CPPNs are said to be geometrically aware because when they compute the weights of the associated ANN, they are given as input the x,y location of both the input and output node in the ANN. Figure duplicated from ~\cite{verbancsics10}.}
\label{fig:cppn}
\end{figure}

The ANN's input representation will be discussed further in Section \ref{sec:interface}. For more information on HyperNEAT, refer to~\cite{gauci08}.

\section{Approach}
\label{sec:approach}
This section describes the components of HyperNEAT-GGP. The main points are the manner in which the raw Atari game screen is processed, the self-agent is identified, and HyperNEAT is interfaced with the detected objects. 

\subsection{Visual Processing}
For nearly any machine learning problem, the question of how to encode the state space is of great importance. Similar to Verbancsics and Stanley's example, HyperNEAT-GGP uses an overhead object representation of the current game screen. Since the Atari provides only the raw pixels of the screen as input, a visual processing stack identifies objects and game entities without a priori knowledge of a specific game~\cite{naddaf10}. A graphical depiction of this stack is shown in Figure \ref{fig:visproc}. While it is likely possible to learn from the raw screen's pixels, object detection requires little work and reduces the complexity of the learning task by eliminating pixels not directly relevant to playing the game.

\begin{figure*}[htp]
\begin{center}
\includegraphics[width=\textwidth]{figures/AtariArch}
\end{center}
\caption{Visual Processing Architecture applied to the game Freeway. Raw pixels from the game screen are displayed on the left. Next, contiguous pixels of the same color are merged into blobs. Objects are then extracted by merging adjacent blobs which exhibit constant velocity over the last two frames. Next, objects are clustered into object classes based on a pixel similarity score. Three main object classes are found -- cars facing left, cars facing right, and the chicken. This approach is similar to that of Naddaf~\cite{naddaf10}. Finally, self detection successfully identifies the chicken blob as the agent and colors it gray (circled in a red dashed line in rightmost screen).}
\label{fig:visproc}
\end{figure*}

Visual processing begins at the raw pixels of the game screen. Image segmentation groups adjacent raw pixels with similar colors into blobs. Next, blob merging occurs, outputting a set of current objects on screen. This process examines all of the recently discovered blobs and compares them with equivalent blobs in the last frame in order to compute a velocity for each blob. Blobs are matched between screens using pixel similarity. Velocity is computed by measuring the displacement of blob centroids. Once each blob is assigned a velocity, adjacent blobs with the same non-zero velocity are merged into objects. Objects that are too small or become stationary are thrown out. This check helps reduce the number of false positives in the object-detection process.

Finally, objects are clustered into object classes or prototypes. Specifically, the shape of each pair of objects is compared and if found to exceed a similarity threshold (97\% pixel match in these experiments), the objects are grouped into the same class. Pixel similarity between two objects is computed by comparing the presence of pixels relative to each object's bounding box. As Figure \ref{fig:visproc} indicates, different object classes are discovered for the cars at the bottom half of the screen, cars at the top half of the screen, and the chicken. 

To reduce the number of spurious prototypes, prototypes are passed to the next stage of the approach only when a number of instances of that prototype are seen within successive frames. In the example in Figure \ref{fig:visproc}, this check helps remove prototypes for objects created when the cars are at the edges of the screen. Prototypes failing this check are removed while those which pass are assigned a unique real number (see Section \ref{sec:interface} for more details).

Objects are assigned to the same class if their shape is relatively similar without taking color into consideration. This assumption has a potential drawback in certain games if different objects have similar shapes but different colors. 

\subsection{Self-Identification}
The self identification step is meant to identify the location of an on-screen entity that is being controlled by the agent. In the vast majority of Atari games, the player's actions affect the movement of some on-screen entity, here termed the \textit{self}. Knowledge of the location of the \textit{self} is crucial to selecting an action, as described below. HyperNEAT-GGP uses an approach based on information gain to identify a blob most likely to correspond to the \textit{self}. Pseudocode is given in Algorithm \ref{alg:idself}.

\begin{algorithm}
\caption{Identify Self}
\label{alg:idself}
\begin{algorithmic}[1]
  \STATE $actions \leftarrow $ set of actions applicable to this game
  \STATE $current\_blobs \leftarrow $ set of blobs in the current game frame
  \STATE $ActionHistory \leftarrow \{a_0...a_n\}$ %\COMMENT{Actions at time 0...n}
  \FOR{blob $b \in current\_blobs$}
  \STATE $vHist_b \leftarrow \{v_0...v_n\}$ %\COMMENT{Velocity of blob $b$ at each timestep in the history}
  %\STATE \COMMENT{Information entropy of $b$'s velocity history}
  \STATE $H_b \leftarrow H(vHist_b)$ 
  \FOR{action $a \in actions$}
  %\STATE \COMMENT{set of $b$'s velocities for timesteps in which action $a$ was taken}
  \STATE $vHist_{(b|a)} \leftarrow [vHist_b[t] ~\forall_t: ActionHist[t-1] == a]$ 
  \STATE $H_{(b|a)} \leftarrow H(vHist_{(b|a)})$ %\COMMENT{Information entropy of $b$'s velocity history given action $a$ was taken}
  \ENDFOR
  \STATE $\textrm{\emph{InfoGain}}_b \leftarrow H_b - sum_{a \in actions}(p_a * H_{(b|a)})$ %\COMMENT{$p_a$ is probability of action $a$ based on observed frequency} 
  \ENDFOR
  \RETURN $arg\_max_{b \in current\_blobs}(\textrm{\emph{InfoGain}}_b)$ %\COMMENT{Return blob with max information gain}
\end{algorithmic}
\end{algorithm}

At the high level, this approach makes certain assumptions about the self entity. First, it is assumed that the self blob will move similarly whenever the same action is performed. That is, whenever an action, say Joystick Up, is taken, the resulting velocity of the self blob should have a similar value (e.g. blob.y\_velocity = -1). 

As input, in lines 1-3, the algorithm has access to the set of possible joystick and button actions applicable to the current game (this is typically a subset of the 18 possible actions present on the Atari console), a list of blobs detected in the current frame, and a history of the actions taken by the agent. Additionally, line 5 assumes access to the $(x,y)$ velocity history $vHist$ of every blob. Next, Algorithm \ref{alg:idself} computes the entropy of blob $b$'s velocity history. Entropy is calculated using the standard formulation: $H(X) = -\sum_{i=1}^n{p(x_i)*ln(p(x_i))}$. Taking entropy over a velocity history involves computing the distribution over blob $b$'s velocity values. This computation is done using the empirically observed frequencies of each observed $(x,y)$ velocity value in $b$'s velocity history. A blob with highly random movement will exhibit a high information entropy over its full velocity history while a blob with highly regular movement will yield low entropy.

Having computed the information entropy over $b$'s full velocity history, Algorithm \ref{alg:idself} next examines each action individually and, in line 8, create $b$'s selective velocity history $vHist_{(b|a)}$. The selective velocity history simply filters the full velocity history by including only velocities that were observed in frames after which action $a$ was taken. For example, if $a = $ joystick left, then $vHist_{(b|a)}$ would only contain resultant velocities for frames in which $a$ was the action selected. In line 9, entropy is computed over $b$'s selective velocity history. This selective entropy $H_{(b|a)}$ should be low if a given action reliably causes the blob to move in a certain direction.

\begin{figure*}
  \centering
  \subfloat[Object Classes]{\label{fig:classes}\includegraphics[width=0.196\textwidth]{figures/objClasses}}\hspace{.1in}
  \subfloat[Grid Overlay]{\label{fig:grid}\includegraphics[width=0.2\textwidth]{figures/grid-overlay}}\hspace{.1in}
  \subfloat[ANN Input]{\label{fig:anninput}\includegraphics[width=0.2\textwidth]{figures/ann-input}}\hspace{.1in}
  \subfloat[ANN Output]{\label{fig:annoutput}\includegraphics[width=0.2\textwidth]{figures/action-selection}}
 \caption{The interface between visual processing framework and the HyperNEAT ANN. Classes of objects are discretized into a grid whose cells are fed to the input nodes of the ANN via a map from object classes to real numbers. After running the ANN, activations of the output layer in cells adjacent to the detected self are used to select which action the agent should take. In this case, the agent would move up since that adjacent cell has the highest activation.}
 \label{fig:interface}
\end{figure*}

Finally, information gain is computed in line 11 by subtracting a frequency weighted sum of a blob's selective velocity entropies from the blob's full velocity entropy. If each of the selective entropies is low, as should be the case for the self blob, a high information gain results. In line 13, the algorithm concludes by returning the blob with maximum information gain.

While Algorithm \ref{alg:idself} is generally successful in identifying the self blob, sometimes game dynamics break the assumption that actions result in similar velocities. For example, in the Freeway game, after colliding with a car, control is taken from the player and the chicken inadvertently is moved down for several frames regardless of which actions the agent is executing. This temporary lack of control results in irregular selective velocity histories and temporarily poorer identification of the self. However, in some sense the algorithm is correct in losing confidence in an object over which it no longer has control. In practice, since the chicken is still the most controlled blob, it remains the self.

\subsection{Atari-HyperNEAT Interface}
\label{sec:interface}
After extracting object classes as well as the location of the self from the raw game screen, this information needs to be sent to HyperNEAT. As discussed in Section \ref{sec:hyperneat}, HyperNEAT evolves a CPPN that encodes an ANN. This section assumes access to a fully connected 2-layer ANN whose weights have been specified by the CPPN. At a high level, information from the game screen needs to be translated to activations of nodes in the input layer of the ANN. Then, after the network has been run in the standard feed-forward fashion, the activation of nodes on the output layer must be interpreted in order to select an action.

Figure \ref{fig:interface} shows an example of how object classes are given as input to the substrate layer of the ANN. Since the ANN input nodes can only take real-valued activations, each class of objects must be mapped to a real number. Thus a mapping from object classes to real values is maintained. Upon discovery of new object classes, real values are incrementally added to the map in a non-decreasing fashion. Next, since the ANN contains a $16\times 21$ grid of input nodes, the raw game screen must also be discretized by a factor of ten in each dimension from the native resolution of $160\times 210$. Figure \ref{fig:grid} shows an example discretization. Following this step, each cell that contains an object is fed as input to the ANN with a real valued activation corresponding to the mapping of that object class. Cells devoid of objects are given input activations of zero. The ANN is run in a standard feed-forward manner, producing activations of the nodes in the $16\times 21$ output layer (Figure \ref{fig:annoutput}). Action selection involves locating the cell corresponding to the detected self object (colored blue in Figure \ref{fig:annoutput}). The activation of this cell as well as the activations of the four adjacent cells (shown with red arrows) are compared and the action corresponding to the arrow in the highest of these five cells is returned (or no-op if the self square has the highest value). This method of action selection supports up to five possible actions. Future work involves extending this framework to support more complex actions such a button presses and combinations of joystick and button-press actions.

%% \subsection{Parallel Implementation}
%% \label{sec:condor}
%% The visual processing and self-identification components of the code require a significant amount of processing time each frame. Depending on the game being played, evaluating a single individual can take anywhere between a few seconds to five minutes. Initial experiments indicated that HyperNEAT would need to run for \textit{at least} 250 generations with 100 individuals per generation to obtain representative results. These parameters make it infeasible to run the entire computation on a single machine in a reasonable amount of time. To speed up evolution, the framework exploits the parallel nature of this approach by evaluating each individual on a separate machine. This framework makes use of the Condor distributed processing cluster \cite{thain2005distributed} to run a massive number of Atari simulations in parallel. A similar framework has been used in the past to speed up the optimization of robot soccer agents in simulation using CMA-ES\cite{Urieli+MKBS:2010}. The HyperNEAT implementation of Verbancsics~\cite{verbancsics10} was also modified to run in parallel on the Condor cluster.

% \begin{figure*}[ht]
% \begin{center}
% \includegraphics[width=\textwidth]{figures/CondorHyperNEAT}
% \end{center}
% \caption{Procedure for using HyperNEAT on the condor framework. An initial Master process is responsible for running NEAT and prodcuding successive generations. Condor jobs are initialized with each job being provided the population file and the id of th individual it is responsible for evaluating. Each job starts an instance of the Atari emulator and proceeds to evaluate the individual by playing the game using the policy defined by that individual. A fitness value is calculated, corresponding to the score obtained in the game and written out to file. This is read back by the Master process which then proceeds to use the fitness values to produce the next generation.}
% \label{fig:condor}
% \end{figure*}

%% \begin{figure}[ht]
%% \begin{center}
%% \includegraphics[width=\columnwidth]{figures/condor-hyperneat-small}
%% \end{center}
%% \caption{Parallel evaluations on the Condor framework.}
%% \label{fig:condor}
%% \end{figure}

%% Figure \ref{fig:condor} details how parallel evaluations are performed using the Condor cluster. Once a generation is produced, all the individuals of that generation are evaluated on separate machines in parallel. After all evaluations have been completed, the fitness values are gathered and a new population is produced. This process is then repeated for each successive generation. 

\section{Experimental Setup}
\label{sec:experiments}
The experimental setup was the same for both Freeway and Asterix: HyperNEAT-GGP was run for 250 generations, with 100 individuals in each generation and a substrate resolution of $16\times 21$. All individuals learn from scratch and were evaluated on the Atari simulator using the same random seed. Evaluations of the 100 individuals in each generation were performed in parallel on a Condor cluster, resulting in each generation taking several minutes of wall-clock time to evaluate. Results are averaged across five runs of HyperNEAT-GGP evolution and compared with previous results obtained using gradient descent Sarsa$(\lambda)$ with linear function approximation\cite{naddaf10}. 

\section{Sarsa-Lambda agents}
\label{sec:sarsa}
Throughout the experimental section, HyperNEAT-GGP is compared against the temporal difference reinforcement learning method Sarsa$(\lambda)$. Due to the large size of the state space, the Sarsa$(\lambda)$ agent uses linear function approximation in which each state-action is represented by a vector of $n$ values, $\Phi(s)$, known as the feature vector, where $n \ll |S|$. To estimate the value of state-action, a vector of parameters $\theta$ is used to create a linear combination as follows: $Q_t(s,a) = \sum_{i=1}^n \theta_t(i) \Phi(s) (i)$.

At each timestep Sarsa$(\lambda)$ applies the following update to all stat-action pairs: $Q_{t+1}(s,a) = Q_t(s,a) + \alpha \delta e_t(s,a)$, where $\delta = r_t + \gamma Q_t(s_{t+1},a_{t+1}) - Q_t(s_t,a_t)$. $e_t(s,a)$ is the eligibility of state-action $(s,a)$ and is decayed by a factor of $\gamma \lambda$ at each timestep, with the exception of the latest state-action $e(s_t,a_t)$, whose eligibility is set to 1. 

Since the agent uses linear function approximation, a vector of eligibility traces is maintained for each item in the feature vector. Similarly, these eligibility traces are decayed by $\gamma \lambda$ in each timestep and set to 1 if the associated feature is active in the current timestep. At each timestep the parameter vector is updated as follows: $\theta_{t+1} = \theta_t + \alpha \delta e_t$ where $\delta$ is defined as before and $\alpha$ is the learning rate. 

Different variations of the Sarsa$(\lambda)$ agent use different feature vectors to represent the state of the Atari game. Sarsa$(\lambda)$-BASS (Basic Abstraction of Screen Shots) discretizes the screen into a binary vector $v_l$ of length 14x18x8 where 14 and 18 are the width and height of the discretized screen and 8 is the number of possible colors in SECAM mode. Next it creates another binary vector $v_q$ which contains the pairwise ANDs of all items in $v_l$. Thus the full feature vector $\Phi_a$ is a $(|v_l| + |v_q|) \times |A|$ bit binary vector representing an abstraction of the raw game screen. 

Sarsa$(\lambda)$-DISCO (Detecting Instances of Class Objects) uses an object detection framework similar to the one outlined in Section \ref{sec:approach} to detect classes of objects present in the current screen. To generate $\Phi$ it encodes the absolute location of each object class on the screen as well as the tile-coded relative position and velocity for each pair of object classes. Since there may be multiple instances of each object class present on a given screen and $\Phi$ is required to have a fixed length, tile-coded positions and velocities are found for each instance and summed. 

Finally, Sarsa$(\lambda)$-RAM uses a state representation based on the 1024 bits of random access memory (RAM) available to the Atari console. A binary vector $v_l$ is created which contains each of the 1024 bits of memory. Next another binary vector $v_q$ encodes the ANDs of all bits in $v_l$. $\Phi$ is a direct concatenation of $v_l$ and $v_q$. For detailed descriptions of these learning agents, see Chapter 2 of \cite{naddaf10}.

\section{Results}
\label{sec:results}
This section provides results obtained from applying HyperNEAT-GGP on two Atari games, \textit{Freeway} and \textit{Asterix}. Table 1 presents the fitness values of the best individual averaged across all the runs, as well as the overall best individual found. Previously published performances of BASS, DISCO, and RAM gradient descent Sarsa$(\lambda)$ agents are also presented as well as an agent taking random actions each step~\cite{naddaf10}. 

\subsection {Freeway}

Freeway is an Atari game similar to the arcade game \textit{Frogger}. The player controls a chicken with the objective of reaching the top of the highway while avoiding cars (Fig. \ref{fig:visproc}). The player receives a score of 1 if the chicken reaches the top of the highway, at which point the chicken moves back to the bottom of the screen. If the chicken gets hit by a car, it gets pushed down by a fixed small distance. The game ends after two minutes of game-play. There are three possible actions that the player may use in Freeway: \textit{Up}, \textit{Down}, and \textit{No-Action}.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=\columnwidth]{figures/freeway-results}
\end{center}
\caption{HyperNEAT-GGP learning performance on the Freeway game. Average fitness of the population along with the champion fitness for each generation are displayed. Error bars represent standard deviation. The fitness of an individual corresponds exactly to its game score. As the figure shows, effective Freeway policies are found in early generations.}
\label{fig:freeway-curve}
\end{figure}

Average fitness of the population and the champion fitness throughout the learning process can be seen in Fig. \ref{fig:freeway-curve}. Results were averaged across five runs of HyperNEAT-GGP evolution. Table 1 contrasts HyperNEAT-GGP with previous results. The most striking observation from these results is that reinforcement learning is unable to find a solution to the problem, whereas even the first generation in HyperNEAT-GGP produces a champion with a very high fitness (starting average champion fitness = 26.4). Sarsa$(\lambda)$ agents would have had to perform a large number of exploratory actions before stumbling on the goal state at the top of the screen, and lacking a single positive reward, Sarsa$(\lambda)$ succumbs to the highly delayed nature of the reward in this game. On the other hand, all individuals in HyperNEAT are produced by randomly initialized CPPNs. Due to the nature of CPPNs, at least a few individuals have the propensity to always take the \textit{Up} action and obtain a large fitness at the outset. Evolution helps individuals to learn quickly to avoid cars over the next 50 generations, which can be seen by the slight increase in champion and average fitness.

\subsection {Asterix}
In Asterix, the player controls a unit called Asterix with the objective of collecting magic potions and avoiding \textit{lyres}. Both the magic potions and the lyres move across the screen around Asterix. Each time Asterix collects a magic potion, the player receives a score of 50. The game is over when the Asterix touches a lyre. Asterix can take five possible actions: \textit{Up}, \textit{Down}, \textit{Left}, \textit{Right}, and \textit{No-Action}.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=\columnwidth]{figures/asterix-results}
\end{center}
\caption{HyperNEAT-GGP learning performance on the Asterix game. The average fitness of the population along with the champion at each generation in the Asterix game. Error bars represent standard deviation. Fitness of an individual corresponds exactly to their game score. As the figure shows, policies are continually improved throughout the course of the 250 generations.}
\label{fig:asterix-curve}
\end{figure}

Average fitness of the population and the champion fitness throughout the learning process can be seen in Fig. \ref{fig:asterix-curve}. Results for Asterix were averaged across 10 runs of HyperNEAT-GGP evolution. Table 1 contrasts our approach with previous results. The results for Asterix are qualitatively different from Freeway in a number of ways: First, random exploration obtains non-zero reward in Asterix as Asterix inadvertently collects magic potions. Reinforcement Learning approaches can bootstrap from this information and learn to become statistically better than random. Additionally, champions from HyperNEAT evolution start at close to random performance (starting average champion fitness = 190) and improve their performance to the same level as that of reinforcement learning approaches within 50 generations. Finally, the learning process steadily improves the fitness through the entirety of 250 generations. This steady improvement demonstrates the power of using CPPNs at representing good policies for this game.

%Results from the Asterix game are summarized in Figure \ref{fig:asterix-curve} and Table 1. Unlike the Freeway game, the champion fitness curve shows significant improvement accross successive generations. Secondly, the champion fitness reaches a high of 750 before dropping down to around the 550 mark. We suspect that our Condor implementaion is responsible, and that the champion was lost to an incomplete Condor job (see Section \ref{sec:condor}). We hope to rectify this issue in the future by making our framework more robust. In contrast to the Freeway game, RL methods perform decently well in this domain. Initial exploration probably causes the agent to learn to stay away from the sprites much faster in this case. However HyperNEAT still outperforms RL based methods. On the other hand, HyperNEAT does not perform as well as a human player. We attribute this to the higher complexity of the game, as well as multiple object classes (which is something we are still working on). 

\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
~ & \textbf{Freeway} & \textbf{Asterix} \\ \hline
\textbf{Sarsa$(\lambda)$-BASS} & 0 & 402 \\ \hline
\textbf{Sarsa$(\lambda)$-DISCO} & 0 & 301 \\ \hline
\textbf{Sarsa$(\lambda)$-RAM} & 0 & 545 \\ \hline
\textbf{Random} & 0 & 156 \\ \hline
\textbf{HyperNEAT-GGP(Average)} & 28.4 & 725 \\ \hline
\textbf{HyperNEAT-GGP(Best)} & 29 & 1000 \\ 
\hline
\end{tabular}
\end{center}
\caption{Game scores obtained in the Freeway and Asterix games. HyperNEAT-GGP substantially outperforms Sarsa$(\lambda)$ on both Freeway and Asterix.}
\end{table}

Results show excellent performance by HyperNEAT-GGP on the Asterix and Freeway games. Informal comparisons with agents controlled by the authors of this paper indicate that HyperNEAT-GGP achieves scores on par with human play. However, to extend HyperNEAT-GGP to play arbitrary games in the Atari simulator, some future work is required.

\section{Future Work}
\label{sec:futurework}
The most pressing direction for future work is to extend HyperNEAT-GGP to a larger set of games. There are three main challenges: large numbers of possible actions, many different object classes, and robust visual processing.

In Freeway and Asterix, like in Robocup Keepaway, there are relatively few classes of objects that matter: cars and the chicken for Freeway, potions and lyres for Asterix, and takers and keepers for Keepaway. With a limited number of object classes it is easy to map from objects to substrate activations. For example, in Keepaway, keepers can be assigned values of 1 and takers values of -1. Having such few values allows HyperNEAT to easily differentiate between classes of objects and exhibit appropriate behaviors for each, such as avoiding lyres and collecting potions.

It is more difficult to differentiate between object classes as the number of classes increases and crowds the map of object class to substrate values. In the worst case, HyperNEAT cannot distinguish between them and formulate appropriate strategies for dealing with each class.

%One possible solution is to create input substrates, with one substrate for each class of objects, as seen in Figure \ref{fig:possiblearch}. Each new object class is given its own substrate layer which it exclusively populates. At run-time, each of the substrate layers is connected to the processing layer and run in a feed-forward manner. To accommodate multiple substrates, an additional input needs to be added to the CPPN to specify which substrate is currently active. In this way, HyperNEAT would, with some additional computational cost, be able to encode an entirely different policy for an arbitrary number of object classes. 

The second area of future work involves developing a better way to handle a large number of actions. In games like Freeway in which there are only a few actions (up, down, no-op), it is possible to choose which action to take by examining values of the output nodes adjacent to the self node (as described in Section \ref{sec:interface}). This issue becomes more complicated when other actions such as button presses are involved. For example, which node's value should be examined to decide if the button press action should be taken? 

%To address this issue, another output layer above the processing layer can be included. This additional output layer would have a single node for each of the possible actions in the game. Action selection would simply reduce to finding the action node with the highest value. If additional actions need to be introduced, more action nodes can be created in the output layer. The top portion of Figure \ref{fig:possiblearch} depicts this alternative architecture.

%% \begin{figure}[htp]
%% \begin{center}
%% \includegraphics[width=\columnwidth]{figures/multiple-substrate.png}
%% \end{center}
%% \caption{Alternative architecture for handling variable numbers of object classes and actions. New object classes are assigned individual substrate layers. Additionally, above the processing level exists a node for each possible action. At run-time activations are propagated up from all $N$ substrate layers to the processing and action layers. Action selection involves picking which action node has the highest activation.}
%% \label{fig:possiblearch}
%% \end{figure}

Finally, the visual processing stack could be made more robust. Specifically, objects are sometimes lost when they change shape or rotate. Additionally unmoving objects such as walls are not detected. This introduces difficulties in games such as Pac-man where static objects are essential to the game dynamics. Finally self-identification could also be improved by incorporating additional information about how long each object has been on-screen, with the assumption that the self object remains on screen while other objects are transitory. Further changes are encountered with self objects which retain velocity -- such as the spaceship in Asteroids. 

Addressing these areas of future work will go a long way towards making HyperNEAT-GGP more generally applicable to Atari games.

\section{Conclusion}
\label{sec:conclusion}
This paper introduces HyperNEAT-GGP, a HyperNEAT-based general Atari game playing agent. Many Atari games contain geometric regularities in the two-dimensional space of the game screen. This structure allows HyperNEAT to quickly learn effective policies. To reduce the complexity of learning from the raw game screen, HyperNEAT-GGP employs a game-independent visual processing hierarchy designed to identify classes of objects as well as the entity that the player controls on the game screen. Identified objects are provided as input to HyperNEAT. Due to the computational overhead of visual processing applied to each game screen, a parallel architecture is used to evaluate multiple individuals simultaneously. Results were presented for two Atari games, \textit{Freeway} and \textit{Asterix}. In both cases, HyperNEAT-GGP was shown to outperform previous reinforcement learning benchmarks~\cite{naddaf10}. While no single Atari game, if studied in isolation and given extensive feature engineering, likely poses too great a challenge for modern AI techniques, the full collection of over 900 Atari games presents a daunting task for a single learning agent. HyperNEAT-GGP represents a first step towards the ambitious goal of creating an agent capable of learning and seamlessly transitioning between many different tasks.

\section{Acknowledgments}
This work has been funded by .....

%\end{document}  % This is where a 'short' article might terminate

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{hyperneat}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references

\end{document}


