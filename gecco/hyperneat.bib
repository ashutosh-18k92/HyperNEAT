@article{Samuel_59, title={Some Studies in Machine Learning Using the Game of Checkers}, volume={3}, url={http://www.research.ibm.com/journal/rd/033/ibmrd0303B.pdf}, number={3}, journal={IBM Journal of Research and Development}, publisher={IBM}, author={Samuel, A L}, year={1959}, pages={210--229}}

@mastersthesis{naddaf10,
  AUTHOR = {Yavar Naddaf},
  TITLE = {Game-Independent AI Agents For Playing Atari 2600 Console Games},
  School= {University of Alberta},
  YEAR = {2010}
}

@article{verbancsics10,
 author = {Verbancsics, Phillip and Stanley, Kenneth O.},
 title = {Evolving Static Representations for Task Transfer},
 journal = {J. Mach. Learn. Res.},
 issue_date = {3/1/2010},
 volume = {11},
 month = {August},
 year = {2010},
 issn = {1532-4435},
 pages = {1737--1769},
 numpages = {33},
 url = {http://dl.acm.org/citation.cfm?id=1756006.1859909},
 acmid = {1859909},
 publisher = {JMLR.org},
} 

@inproceedings{gauci08,
    abstract = {{An important feature of many problem domains in machine learning is their geometry. For example, adjacency relationships, symmetries, and Cartesian coordinates are essential to any complete description of board games, visual recognition, or vehicle control. Yet many approaches to learning ignore such information in their representations, instead inputting flat parameter vectors with no indication of how those parameters are situated geometrically. This paper argues that such geometric information is critical to the ability of any machine learning approach to effectively generalize; even a small shift in the configuration of the task in space from what was experienced in training can go wholly unrecognized unless the algorithm is able to learn the regularities in decision-making across the problem geometry. To demonstrate the importance of learning from geometry, three variants of the same evolutionary learning algorithm (NeuroEvolution of Augmenting Topologies), whose representations vary in their capacity to encode geometry, are compared in checkers. The result is that the variant that can learn geometric regularities produces a significantly more general solution. The conclusion is that it is important to enable machine learning to detect and thereby learn from the geometry of its problems.}},
    author = {Gauci, Jason and Stanley, Kenneth O.},
    booktitle = {Proceedings of the 23rd National Conference on Artificial Intelligence (AAAI)},
    citeulike-article-id = {9744476},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1620169},
    keywords = {evolution, machine\_learning},
    posted-at = {2011-09-06 17:36:17},
    priority = {2},
    title = {{A case study on the critical role of geometric regularity in machine learning}},
    url = {http://portal.acm.org/citation.cfm?id=1620169},
    year = {2008}
}

@inproceedings{ambrosio08,
    abstract = {{This paper argues that multiagent learning is a potential "killer application" for generative and developmental systems (GDS) because key challenges in learning to coordinate a team of agents are naturally addressed through indirect encodings and information reuse. For example, a significant problem for multiagent learning is that policies learned separately for different agent roles may nevertheless need to share a basic skill set, forcing the learning algorithm to reinvent the wheel for each agent. GDS is a good match for this kind of problem because it specializes in ways to encode patterns of related yet varying motifs. In this paper, to establish the promise of this capability, the Hypercube-based NeuroEvolution of Augmenting Topologies (HyperNEAT) generative approach to evolving neurocontrollers learns a set of coordinated policies encoded by a  single  genome representing a team of predator agents that work together to capture prey. Experimental results show that it is not only possible, but beneficial to encode a heterogeneous team of agents with an indirect encoding. The main contribution is thus to open up a significant new application domain for GDS.}},
    address = {New York, NY, USA},
    author = {D'Ambrosio, David B. and Stanley, Kenneth O.},
    booktitle = {GECCO '08: Proceedings of the 10th annual conference on Genetic and evolutionary computation},
    citeulike-article-id = {6235855},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1389095.1389256},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1389095.1389256},
    doi = {10.1145/1389095.1389256},
    isbn = {978-1-60558-130-9},
    keywords = {alife10, behavior, evolution, indirect},
    location = {Atlanta, GA, USA},
    pages = {819--826},
    posted-at = {2010-01-27 10:51:11},
    priority = {2},
    publisher = {ACM},
    title = {{Generative encoding for multiagent learning}},
    url = {http://dx.doi.org/10.1145/1389095.1389256},
    year = {2008}
}

@inproceedings{clune09,
 author = {Clune, Jeff and Beckmann, Benjamin E. and Ofria, Charles and Pennock, Robert T.},
 title = {Evolving coordinated quadruped gaits with the HyperNEAT generative encoding},
 booktitle = {Proceedings of the Eleventh conference on Congress on Evolutionary Computation},
 series = {CEC'09},
 year = {2009},
 isbn = {978-1-4244-2958-5},
 location = {Trondheim, Norway},
 pages = {2764--2771},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=1689599.1689966},
 acmid = {1689966},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
} 

@Article{stanley02,
title={Evolving Neural Networks Through Augmenting Topologies},
author={Kenneth O. Stanley and Risto Miikkulainen},
volume={10},
journal={Evolutionary Computation},
number={2},
pages={99-127},
url="http://nn.cs.utexas.edu/?stanley:ec02",
year={2002}
}

@InProceedings(stone01,
	Author="Peter Stone and Richard S. Sutton",
	Title="Scaling Reinforcement Learning toward {R}obo{C}up Soccer",
	BookTitle="Proceedings of the Eighteenth International Conference on Machine Learning",
    publisher = "Morgan Kaufmann, San Francisco, CA",
    pages = "537--544",
    year = "2001",
    abstract={
              RoboCup simulated soccer presents many challenges to
              reinforcement learning methods, including a large state
              space, hidden and uncertain state, multiple agents, and
              long and variable delays in the effects of actions.  We
              describe our application of episodic SMDP
              Sarsa(lambda)with linear tile-coding function
              approximation and variable lambda to learning
              higher-level decisions in a keepaway subtask of RoboCup
              soccer.  In keepaway, one team, ``the keepers,'' tries
              to keep control of the ball for as long as possible
              despite the efforts of ``the takers.''  The keepers
              learn individually when to hold the ball and when to
              pass to a teammate, while the takers learn when to
              charge the ball-holder and when to cover possible
              passing lanes.  Our agents learned policies that
              significantly out-performed a range of benchmark
              policies.  We demonstrate the generality of our approach
              by applying it to a number of task variations including
              different field sizes and different numbers of players
              on each team.
    },
    wwwnote={<a href="http://www.ecn.purdue.edu/ICML2001/">ICML-2001</a><br>
            Some <a href="http://www.cs.utexas.edu/users/AustinVilla/sim/keepaway/">simulations of keepaway</a> referenced in the paper.},
)

@ARTICLE{genesereth05,
    author = {Michael Genesereth and Nathaniel Love},
    title = {General game playing: Overview of the AAAI competition},
    journal = {AI Magazine},
    year = {2005},
    volume = {26},
    pages = {62--72}
}

@inproceedings{duik08,
    abstract = {{Rich representations in reinforcement learning have been studied for the purpose of enabling generalization and making learning feasible in large state spaces. We introduce Object-Oriented MDPs (OO-MDPs), a representation based on objects and their interactions, which is a natural way of modeling environments and offers important generalization opportunities. We introduce a learning algorithm for deterministic OO-MDPs and prove a polynomial bound on its sample complexity. We illustrate the performance gains of our representation and algorithm in the wellknown Taxi domain, plus a real-life videogame.}},
    author = {Diuk, Carlos and Cohen, Andre and Littman, Michael L.},
    booktitle = {Proceedings of 25th International Conference on Machine Learning (ICML)},
    citeulike-article-id = {7358711},
    citeulike-linkout-0 = {http://paul.rutgers.edu/\~{}cdiuk/papers/OORL.pdf},
    citeulike-linkout-1 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.149.7056},
    keywords = {learning, phd, planning, relational-rl},
    pages = {240--247},
    posted-at = {2010-06-25 10:25:50},
    priority = {0},
    title = {{An object-oriented representation for efficient reinforcement learning}},
    url = {http://paul.rutgers.edu/\~{}cdiuk/papers/OORL.pdf},
    year = {2008}
}

@misc{atarihist,
Author = {Geoff Edgers},
Title = {Atari and the deep history of video games},
howpublished = {\url{http://www.boston.com/bostonglobe/ideas/articles/2009/03/08/a_talk_with_nick_montfort/}} 
}

@misc{pacmancompetition,
Author = {Simon M. Lucas},
Title = {Ms Pac-Man Competition (screen capture mode)},
howpublished = {\url{http://dces.essex.ac.uk/staff/sml/pacman/CIG2011Results.html}} 
}

@ARTICLE{sigevolution2007,
    author = {Simon M. Lucas},
    title = {Ms Pac-Man Competition},
    journal = {SIGEVOlution},
    year = {2007},
    volume = {2},
    number = {4},
    pages = {37--38}
}
@InProceedings{Urieli+MKBS:2010,
  author =       "Urieli, Daniel and MacAlpine, Patrick and Kalyanakrishnan, Shivaram and Bentor, Yinon and Stone, Peter",
  title =        "On Optimizing Interdependent Skills: A Case Study in Simulated 3D Humanoid Robot Soccer",
  booktitle =    "Proceedings of the Tenth International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2011)",
  year =         "2011",
  ISBN =         "978-0-9826571-5-7",
  editor =    "Tumer, Kagan and Yolum, Pinar and Sonenberg, Liz and Stone, Peter",
  volume =    "2",
  publisher = "IFAAMAS",
  pages =     "769--776",
}

@article{thain2005distributed,
  title={Distributed computing in practice: The Condor experience},
  author={Thain, D. and Tannenbaum, T. and Livny, M.},
  journal={Concurrency and Computation: Practice and Experience},
  volume={17},
  number={2-4},
  pages={323--356},
  year={2005},
  publisher={Wiley Online Library}
}

@article{tesauro_94,
 author = {Tesauro, Gerald},
 title = {TD-Gammon, a self-teaching backgammon program, achieves master-level play},
 journal = {Neural Comput.},
 volume = {6},
 issue = {2},
 month = {March},
 year = {1994},
 issn = {0899-7667},
 pages = {215--219},
 numpages = {5},
 url = {http://dl.acm.org/citation.cfm?id=188104.188107},
 doi = {10.1162/neco.1994.6.2.215},
 acmid = {188107},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@Article{stone05,
        Author="Peter Stone and Richard S. Sutton and Gregory Kuhlmann",
        Title="Reinforcement Learning for {R}obo{C}up-Soccer Keepaway",
        journal="Adaptive Behavior",
	volume="13",number="3",
        year = "2005", pages="165--188",
    abstract={
              RoboCup simulated soccer presents many challenges to
              reinforcement learning methods, including a large state
              space, hidden and uncertain state, multiple independent
              agents learning simultaneously, and long and variable
              delays in the effects of actions.  We describe our
              application of episodic SMDP Sarsa(lambda) with linear
              tile-coding function approximation and variable
              lambda to learning higher-level decisions in a
              keepaway subtask of RoboCup soccer.  In keepaway, one
              team, ``the keepers,'' tries to keep control of the ball
              for as long as possible despite the efforts of ``the
              takers.''  The keepers learn individually when to hold
              the ball and when to pass to a teammate. Our agents
              learned policies that significantly outperform a range
              of benchmark policies.  We demonstrate the generality of
              our approach by applying it to a number of task
              variations including different field sizes and different
              numbers of players on each team.
    },
    wwwnote={Contains material that was previously published in an <a href="http://www.cs.utexas.edu/~pstone/Papers/2001ml/keepaway.pdf">ICML-2001 paper </a> and a <a href="http://www.cs.utexas.edu/~pstone/Papers/2003robocup/keepaway-progress.pdf"> RoboCup 2003 Symposium paper</a>.<br>
            Some <a href="http://www.cs.utexas.edu/users/AustinVilla/sim/keepaway/">simulations of keepaway</a> referenced in the paper and keepaway software.},
} 